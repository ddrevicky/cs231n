{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.369078\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Since there are 10 classes the probability of randomly guessing the correct class is 0.1 and that is what we would expect if we randomly initialize the weights.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.729799 analytic: -1.729799, relative error: 6.068224e-08\n",
      "numerical: -0.733029 analytic: -0.733029, relative error: 9.076980e-08\n",
      "numerical: -2.981267 analytic: -2.981267, relative error: 3.593139e-09\n",
      "numerical: -0.064475 analytic: -0.064475, relative error: 3.868887e-07\n",
      "numerical: 0.295914 analytic: 0.295914, relative error: 1.085915e-07\n",
      "numerical: 0.393906 analytic: 0.393906, relative error: 1.480902e-07\n",
      "numerical: -0.751775 analytic: -0.751776, relative error: 1.363017e-07\n",
      "numerical: 0.975939 analytic: 0.975939, relative error: 7.023532e-09\n",
      "numerical: 0.512904 analytic: 0.512904, relative error: 1.949043e-08\n",
      "numerical: 0.280249 analytic: 0.280249, relative error: 7.054118e-08\n",
      "numerical: 0.649556 analytic: 0.649556, relative error: 5.919668e-08\n",
      "numerical: -1.680619 analytic: -1.680619, relative error: 5.281864e-09\n",
      "numerical: -0.640812 analytic: -0.640812, relative error: 2.435711e-08\n",
      "numerical: -1.318516 analytic: -1.318516, relative error: 6.976795e-08\n",
      "numerical: -5.688533 analytic: -5.688533, relative error: 6.409319e-09\n",
      "numerical: 3.374989 analytic: 3.374989, relative error: 4.162745e-08\n",
      "numerical: 3.107846 analytic: 3.107846, relative error: 1.243798e-08\n",
      "numerical: -1.827008 analytic: -1.827008, relative error: 2.075768e-09\n",
      "numerical: -0.824352 analytic: -0.824352, relative error: 3.930981e-08\n",
      "numerical: -1.523011 analytic: -1.523011, relative error: 9.391759e-09\n",
      "numerical: 3.701127 analytic: 3.701127, relative error: 1.431835e-08\n",
      "numerical: 1.684194 analytic: 1.684194, relative error: 4.459274e-08\n",
      "numerical: -0.999998 analytic: -0.999998, relative error: 7.459383e-09\n",
      "numerical: 1.272570 analytic: 1.272570, relative error: 1.090150e-08\n",
      "numerical: 0.404593 analytic: 0.404593, relative error: 9.962723e-09\n",
      "numerical: 1.909351 analytic: 1.909351, relative error: 3.211451e-09\n",
      "numerical: -0.456692 analytic: -0.456692, relative error: 8.687153e-08\n",
      "numerical: -0.747586 analytic: -0.747586, relative error: 8.630180e-08\n",
      "numerical: 2.074231 analytic: 2.074231, relative error: 1.994368e-08\n",
      "numerical: 2.223794 analytic: 2.223794, relative error: 4.912901e-08\n",
      "numerical: 1.512386 analytic: 1.512385, relative error: 3.243395e-08\n",
      "numerical: 0.123741 analytic: 0.123741, relative error: 2.321047e-07\n",
      "numerical: 0.850757 analytic: 0.850757, relative error: 1.785477e-08\n",
      "numerical: 0.656803 analytic: 0.656803, relative error: 9.273950e-08\n",
      "numerical: 0.979210 analytic: 0.979210, relative error: 5.062701e-08\n",
      "numerical: -0.593277 analytic: -0.593277, relative error: 9.654211e-09\n",
      "numerical: -3.914721 analytic: -3.914721, relative error: 1.876278e-09\n",
      "numerical: -1.286224 analytic: -1.286224, relative error: 4.867695e-09\n",
      "numerical: -0.280394 analytic: -0.280394, relative error: 1.240455e-07\n",
      "numerical: 0.935930 analytic: 0.935930, relative error: 5.550183e-08\n",
      "numerical: -1.015492 analytic: -1.015492, relative error: 2.855479e-08\n",
      "numerical: 0.024704 analytic: 0.024703, relative error: 3.414224e-06\n",
      "numerical: 1.816458 analytic: 1.816458, relative error: 5.029596e-08\n",
      "numerical: 1.814982 analytic: 1.814982, relative error: 2.026751e-08\n",
      "numerical: -3.290067 analytic: -3.290068, relative error: 2.834625e-08\n",
      "numerical: 4.065561 analytic: 4.065560, relative error: 3.230482e-08\n",
      "numerical: -0.379337 analytic: -0.379337, relative error: 9.686785e-08\n",
      "numerical: 0.748262 analytic: 0.748261, relative error: 8.311425e-08\n",
      "numerical: 0.271369 analytic: 0.271369, relative error: 1.605535e-07\n",
      "numerical: 2.941071 analytic: 2.941071, relative error: 4.737308e-08\n",
      "numerical: 1.243906 analytic: 1.243906, relative error: 4.047874e-09\n",
      "numerical: 0.135293 analytic: 0.135293, relative error: 5.603551e-09\n",
      "numerical: -0.113069 analytic: -0.113069, relative error: 4.671808e-07\n",
      "numerical: -0.726207 analytic: -0.726207, relative error: 9.974358e-08\n",
      "numerical: -3.007801 analytic: -3.007801, relative error: 2.865351e-10\n",
      "numerical: -3.924300 analytic: -3.924300, relative error: 2.957994e-09\n",
      "numerical: 4.684230 analytic: 4.684230, relative error: 1.353312e-08\n",
      "numerical: 0.953037 analytic: 0.953037, relative error: 1.934245e-08\n",
      "numerical: 2.667597 analytic: 2.667597, relative error: 3.309547e-08\n",
      "numerical: -0.602046 analytic: -0.602046, relative error: 8.573729e-08\n",
      "numerical: 0.990440 analytic: 0.990440, relative error: 4.124007e-08\n",
      "numerical: 2.025129 analytic: 2.025129, relative error: 6.860302e-09\n",
      "numerical: 0.714487 analytic: 0.714487, relative error: 1.028421e-08\n",
      "numerical: 1.688882 analytic: 1.688882, relative error: 3.929371e-08\n",
      "numerical: -1.327624 analytic: -1.327624, relative error: 2.093075e-09\n",
      "numerical: 1.060607 analytic: 1.060607, relative error: 8.495138e-11\n",
      "numerical: -0.103313 analytic: -0.103313, relative error: 4.258222e-07\n",
      "numerical: -0.759054 analytic: -0.759054, relative error: 6.556431e-08\n",
      "numerical: 3.428834 analytic: 3.428834, relative error: 1.896178e-08\n",
      "numerical: 0.663786 analytic: 0.663786, relative error: 3.538369e-08\n",
      "numerical: 0.738085 analytic: 0.738085, relative error: 1.829485e-08\n",
      "numerical: -1.565481 analytic: -1.565481, relative error: 5.812187e-09\n",
      "numerical: 2.144745 analytic: 2.144745, relative error: 2.385254e-08\n",
      "numerical: -2.880945 analytic: -2.880945, relative error: 8.223575e-10\n",
      "numerical: 0.928225 analytic: 0.928225, relative error: 4.472044e-08\n",
      "numerical: -0.644389 analytic: -0.644389, relative error: 7.043885e-08\n",
      "numerical: -5.021240 analytic: -5.021240, relative error: 3.908764e-09\n",
      "numerical: 0.157738 analytic: 0.157738, relative error: 2.454077e-07\n",
      "numerical: -0.097235 analytic: -0.097235, relative error: 4.796666e-08\n",
      "numerical: -1.565717 analytic: -1.565717, relative error: 1.849984e-09\n",
      "numerical: -0.012394 analytic: -0.012395, relative error: 2.498348e-06\n",
      "numerical: -4.868042 analytic: -4.868042, relative error: 7.243164e-10\n",
      "numerical: 1.060607 analytic: 1.060607, relative error: 8.495138e-11\n",
      "numerical: 0.019981 analytic: 0.019981, relative error: 3.105603e-06\n",
      "numerical: 2.405060 analytic: 2.405060, relative error: 1.823632e-08\n",
      "numerical: -4.678322 analytic: -4.678322, relative error: 3.369256e-09\n",
      "numerical: -2.492941 analytic: -2.492941, relative error: 3.293090e-09\n",
      "numerical: -2.886172 analytic: -2.886172, relative error: 8.622467e-09\n",
      "numerical: 2.423483 analytic: 2.423483, relative error: 1.138485e-08\n",
      "numerical: -0.649977 analytic: -0.649977, relative error: 1.487109e-09\n",
      "numerical: -0.085572 analytic: -0.085572, relative error: 5.838146e-07\n",
      "numerical: 0.678981 analytic: 0.678981, relative error: 3.378172e-08\n",
      "numerical: 3.588779 analytic: 3.588779, relative error: 1.044464e-08\n",
      "numerical: -2.922766 analytic: -2.922767, relative error: 3.493267e-08\n",
      "numerical: 1.517134 analytic: 1.517134, relative error: 1.563451e-08\n",
      "numerical: -1.369339 analytic: -1.369339, relative error: 6.378554e-08\n",
      "numerical: -1.633032 analytic: -1.633032, relative error: 5.665347e-08\n",
      "numerical: 2.258223 analytic: 2.258223, relative error: 2.727443e-08\n",
      "numerical: 1.543914 analytic: 1.543913, relative error: 3.583479e-08\n",
      "numerical: -1.556539 analytic: -1.556539, relative error: 3.452554e-08\n",
      "numerical: -0.115300 analytic: -0.115300, relative error: 1.008193e-07\n",
      "numerical: 4.323862 analytic: 4.323861, relative error: 2.623363e-08\n",
      "numerical: 2.087350 analytic: 2.087350, relative error: 3.220468e-10\n",
      "numerical: -0.997035 analytic: -0.997035, relative error: 6.858221e-08\n",
      "numerical: 2.500150 analytic: 2.500150, relative error: 1.025039e-08\n",
      "numerical: 3.450678 analytic: 3.450678, relative error: 1.118960e-08\n",
      "numerical: 2.174702 analytic: 2.174702, relative error: 2.791227e-08\n",
      "numerical: -1.372167 analytic: -1.372167, relative error: 1.309981e-08\n",
      "numerical: -0.153186 analytic: -0.153186, relative error: 1.495843e-07\n",
      "numerical: 0.457611 analytic: 0.457611, relative error: 1.959055e-08\n",
      "numerical: -0.941258 analytic: -0.941258, relative error: 1.705634e-08\n",
      "numerical: 0.027451 analytic: 0.027451, relative error: 8.669328e-07\n",
      "numerical: 0.463408 analytic: 0.463408, relative error: 9.955752e-09\n",
      "numerical: 1.744202 analytic: 1.744202, relative error: 3.295882e-08\n",
      "numerical: 0.738887 analytic: 0.738887, relative error: 2.253904e-08\n",
      "numerical: 0.517846 analytic: 0.517846, relative error: 3.970425e-08\n",
      "numerical: -3.686791 analytic: -3.686791, relative error: 7.319084e-10\n",
      "numerical: -0.431413 analytic: -0.431413, relative error: 1.144066e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.904234 analytic: 0.904234, relative error: 7.676441e-08\n",
      "numerical: 0.923783 analytic: 0.923783, relative error: 6.780229e-08\n",
      "numerical: 0.122042 analytic: 0.122042, relative error: 1.568341e-07\n",
      "numerical: -0.681305 analytic: -0.681305, relative error: 4.089310e-08\n",
      "numerical: 0.597450 analytic: 0.597450, relative error: 2.175954e-08\n",
      "numerical: 0.335142 analytic: 0.335142, relative error: 1.521166e-07\n",
      "numerical: -0.449182 analytic: -0.449182, relative error: 2.478933e-08\n",
      "numerical: 3.706463 analytic: 3.706463, relative error: 1.450976e-08\n",
      "numerical: -4.071447 analytic: -4.071447, relative error: 4.763510e-09\n",
      "numerical: 0.783007 analytic: 0.783007, relative error: 5.308478e-10\n",
      "numerical: -1.139713 analytic: -1.139713, relative error: 3.838565e-09\n",
      "numerical: -1.277489 analytic: -1.277489, relative error: 1.550079e-08\n",
      "numerical: 2.365953 analytic: 2.365953, relative error: 1.406651e-08\n",
      "numerical: -1.269312 analytic: -1.269312, relative error: 3.872686e-08\n",
      "numerical: 2.163295 analytic: 2.163295, relative error: 2.155238e-08\n",
      "numerical: -3.384562 analytic: -3.384562, relative error: 6.751963e-09\n",
      "numerical: 1.927717 analytic: 1.927716, relative error: 7.056641e-08\n",
      "numerical: 1.116576 analytic: 1.116575, relative error: 6.437292e-08\n",
      "numerical: -4.668661 analytic: -4.668661, relative error: 1.279552e-08\n",
      "numerical: -0.278653 analytic: -0.278653, relative error: 4.338809e-08\n",
      "numerical: -0.749818 analytic: -0.749819, relative error: 2.623973e-08\n",
      "numerical: 0.833510 analytic: 0.833510, relative error: 4.152454e-08\n",
      "numerical: 2.455458 analytic: 2.455457, relative error: 3.484424e-08\n",
      "numerical: 0.811139 analytic: 0.811139, relative error: 2.553446e-08\n",
      "numerical: 0.231298 analytic: 0.231298, relative error: 1.141586e-07\n",
      "numerical: 0.099975 analytic: 0.099975, relative error: 5.002292e-07\n",
      "numerical: 3.581056 analytic: 3.581056, relative error: 1.428288e-08\n",
      "numerical: 2.190607 analytic: 2.190607, relative error: 3.661353e-08\n",
      "numerical: 4.395595 analytic: 4.395595, relative error: 2.319913e-08\n",
      "numerical: -1.291853 analytic: -1.291853, relative error: 4.996144e-09\n",
      "numerical: 0.575935 analytic: 0.575935, relative error: 3.069930e-08\n",
      "numerical: -3.237662 analytic: -3.237662, relative error: 2.448259e-08\n",
      "numerical: 1.031864 analytic: 1.031864, relative error: 1.783767e-08\n",
      "numerical: -1.006074 analytic: -1.006074, relative error: 7.317366e-09\n",
      "numerical: -1.820966 analytic: -1.820966, relative error: 5.860025e-09\n",
      "numerical: 0.980204 analytic: 0.980204, relative error: 5.139341e-08\n",
      "numerical: -1.365143 analytic: -1.365143, relative error: 4.171618e-08\n",
      "numerical: 2.368315 analytic: 2.368315, relative error: 3.938961e-08\n",
      "numerical: 0.056575 analytic: 0.056575, relative error: 1.371931e-06\n",
      "numerical: -1.539078 analytic: -1.539078, relative error: 7.078393e-09\n",
      "numerical: -2.076232 analytic: -2.076232, relative error: 1.443389e-08\n",
      "numerical: 1.229324 analytic: 1.229324, relative error: 3.266509e-08\n",
      "4.56 s ± 108 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.369078e+00 computed in 0.113781s\n",
      "vectorized loss: 2.369078e+00 computed in 0.004438s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 5.000000e-08 reg 5.000000e+02\n",
      "iteration 0 / 5500: loss 21.799008\n",
      "iteration 100 / 5500: loss 20.321793\n",
      "iteration 200 / 5500: loss 19.074057\n",
      "iteration 300 / 5500: loss 18.647829\n",
      "iteration 400 / 5500: loss 18.412395\n",
      "iteration 500 / 5500: loss 18.158774\n",
      "iteration 600 / 5500: loss 17.831536\n",
      "iteration 700 / 5500: loss 17.481582\n",
      "iteration 800 / 5500: loss 17.422148\n",
      "iteration 900 / 5500: loss 17.190031\n",
      "iteration 1000 / 5500: loss 16.887033\n",
      "iteration 1100 / 5500: loss 16.601889\n",
      "iteration 1200 / 5500: loss 16.669770\n",
      "iteration 1300 / 5500: loss 16.456929\n",
      "iteration 1400 / 5500: loss 16.106188\n",
      "iteration 1500 / 5500: loss 15.842596\n",
      "iteration 1600 / 5500: loss 15.812531\n",
      "iteration 1700 / 5500: loss 15.422581\n",
      "iteration 1800 / 5500: loss 15.379338\n",
      "iteration 1900 / 5500: loss 15.176415\n",
      "iteration 2000 / 5500: loss 15.141256\n",
      "iteration 2100 / 5500: loss 15.079569\n",
      "iteration 2200 / 5500: loss 14.845562\n",
      "iteration 2300 / 5500: loss 14.469285\n",
      "iteration 2400 / 5500: loss 14.358573\n",
      "iteration 2500 / 5500: loss 14.370719\n",
      "iteration 2600 / 5500: loss 14.228562\n",
      "iteration 2700 / 5500: loss 14.134854\n",
      "iteration 2800 / 5500: loss 13.796021\n",
      "iteration 2900 / 5500: loss 13.772156\n",
      "iteration 3000 / 5500: loss 13.613647\n",
      "iteration 3100 / 5500: loss 13.639803\n",
      "iteration 3200 / 5500: loss 13.286166\n",
      "iteration 3300 / 5500: loss 13.283882\n",
      "iteration 3400 / 5500: loss 13.152647\n",
      "iteration 3500 / 5500: loss 12.879017\n",
      "iteration 3600 / 5500: loss 12.568762\n",
      "iteration 3700 / 5500: loss 12.639555\n",
      "iteration 3800 / 5500: loss 12.594244\n",
      "iteration 3900 / 5500: loss 12.653538\n",
      "iteration 4000 / 5500: loss 12.401638\n",
      "iteration 4100 / 5500: loss 12.435890\n",
      "iteration 4200 / 5500: loss 12.189800\n",
      "iteration 4300 / 5500: loss 12.045467\n",
      "iteration 4400 / 5500: loss 11.901435\n",
      "iteration 4500 / 5500: loss 11.962822\n",
      "iteration 4600 / 5500: loss 11.561365\n",
      "iteration 4700 / 5500: loss 11.533962\n",
      "iteration 4800 / 5500: loss 11.420507\n",
      "iteration 4900 / 5500: loss 11.311252\n",
      "iteration 5000 / 5500: loss 11.273550\n",
      "iteration 5100 / 5500: loss 11.336588\n",
      "iteration 5200 / 5500: loss 11.010611\n",
      "iteration 5300 / 5500: loss 10.906680\n",
      "iteration 5400 / 5500: loss 10.823555\n",
      "lr 5.000000e-08 reg 1.000000e+03\n",
      "iteration 0 / 5500: loss 36.485467\n",
      "iteration 100 / 5500: loss 34.416221\n",
      "iteration 200 / 5500: loss 33.585198\n",
      "iteration 300 / 5500: loss 32.418700\n",
      "iteration 400 / 5500: loss 31.572162\n",
      "iteration 500 / 5500: loss 30.925797\n",
      "iteration 600 / 5500: loss 30.401304\n",
      "iteration 700 / 5500: loss 29.685516\n",
      "iteration 800 / 5500: loss 29.027210\n",
      "iteration 900 / 5500: loss 28.603005\n",
      "iteration 1000 / 5500: loss 27.910082\n",
      "iteration 1100 / 5500: loss 27.137749\n",
      "iteration 1200 / 5500: loss 26.639166\n",
      "iteration 1300 / 5500: loss 26.172527\n",
      "iteration 1400 / 5500: loss 25.519481\n",
      "iteration 1500 / 5500: loss 25.268594\n",
      "iteration 1600 / 5500: loss 24.540302\n",
      "iteration 1700 / 5500: loss 24.233417\n",
      "iteration 1800 / 5500: loss 23.603497\n",
      "iteration 1900 / 5500: loss 23.204161\n",
      "iteration 2000 / 5500: loss 22.881624\n",
      "iteration 2100 / 5500: loss 22.150866\n",
      "iteration 2200 / 5500: loss 21.878352\n",
      "iteration 2300 / 5500: loss 21.354237\n",
      "iteration 2400 / 5500: loss 21.080146\n",
      "iteration 2500 / 5500: loss 20.740680\n",
      "iteration 2600 / 5500: loss 20.393198\n",
      "iteration 2700 / 5500: loss 20.012018\n",
      "iteration 2800 / 5500: loss 19.580092\n",
      "iteration 2900 / 5500: loss 19.087589\n",
      "iteration 3000 / 5500: loss 18.850746\n",
      "iteration 3100 / 5500: loss 18.443427\n",
      "iteration 3200 / 5500: loss 18.210217\n",
      "iteration 3300 / 5500: loss 17.668696\n",
      "iteration 3400 / 5500: loss 17.637088\n",
      "iteration 3500 / 5500: loss 17.123075\n",
      "iteration 3600 / 5500: loss 16.857930\n",
      "iteration 3700 / 5500: loss 16.490752\n",
      "iteration 3800 / 5500: loss 16.217960\n",
      "iteration 3900 / 5500: loss 15.928995\n",
      "iteration 4000 / 5500: loss 15.657565\n",
      "iteration 4100 / 5500: loss 15.316211\n",
      "iteration 4200 / 5500: loss 14.955551\n",
      "iteration 4300 / 5500: loss 14.761630\n",
      "iteration 4400 / 5500: loss 14.474128\n",
      "iteration 4500 / 5500: loss 14.156051\n",
      "iteration 4600 / 5500: loss 14.062094\n",
      "iteration 4700 / 5500: loss 13.761941\n",
      "iteration 4800 / 5500: loss 13.538352\n",
      "iteration 4900 / 5500: loss 13.332057\n",
      "iteration 5000 / 5500: loss 12.872046\n",
      "iteration 5100 / 5500: loss 12.884318\n",
      "iteration 5200 / 5500: loss 12.594789\n",
      "iteration 5300 / 5500: loss 12.333286\n",
      "iteration 5400 / 5500: loss 12.129558\n",
      "lr 5.000000e-08 reg 5.000000e+03\n",
      "iteration 0 / 5500: loss 160.864102\n",
      "iteration 100 / 5500: loss 144.509295\n",
      "iteration 200 / 5500: loss 130.261414\n",
      "iteration 300 / 5500: loss 117.847778\n",
      "iteration 400 / 5500: loss 106.853237\n",
      "iteration 500 / 5500: loss 96.571469\n",
      "iteration 600 / 5500: loss 87.415757\n",
      "iteration 700 / 5500: loss 78.940220\n",
      "iteration 800 / 5500: loss 71.901512\n",
      "iteration 900 / 5500: loss 65.017040\n",
      "iteration 1000 / 5500: loss 58.959269\n",
      "iteration 1100 / 5500: loss 53.412148\n",
      "iteration 1200 / 5500: loss 48.656959\n",
      "iteration 1300 / 5500: loss 44.140816\n",
      "iteration 1400 / 5500: loss 40.039035\n",
      "iteration 1500 / 5500: loss 36.275334\n",
      "iteration 1600 / 5500: loss 33.090326\n",
      "iteration 1700 / 5500: loss 30.091031\n",
      "iteration 1800 / 5500: loss 27.333182\n",
      "iteration 1900 / 5500: loss 24.943361\n",
      "iteration 2000 / 5500: loss 22.680633\n",
      "iteration 2100 / 5500: loss 20.766388\n",
      "iteration 2200 / 5500: loss 18.841934\n",
      "iteration 2300 / 5500: loss 17.287403\n",
      "iteration 2400 / 5500: loss 15.800108\n",
      "iteration 2500 / 5500: loss 14.571746\n",
      "iteration 2600 / 5500: loss 13.291925\n",
      "iteration 2700 / 5500: loss 12.187644\n",
      "iteration 2800 / 5500: loss 11.213705\n",
      "iteration 2900 / 5500: loss 10.246718\n",
      "iteration 3000 / 5500: loss 9.586498\n",
      "iteration 3100 / 5500: loss 8.817248\n",
      "iteration 3200 / 5500: loss 8.111407\n",
      "iteration 3300 / 5500: loss 7.613864\n",
      "iteration 3400 / 5500: loss 6.947776\n",
      "iteration 3500 / 5500: loss 6.450089\n",
      "iteration 3600 / 5500: loss 6.227583\n",
      "iteration 3700 / 5500: loss 5.670091\n",
      "iteration 3800 / 5500: loss 5.364985\n",
      "iteration 3900 / 5500: loss 4.997292\n",
      "iteration 4000 / 5500: loss 4.769785\n",
      "iteration 4100 / 5500: loss 4.483416\n",
      "iteration 4200 / 5500: loss 4.136156\n",
      "iteration 4300 / 5500: loss 3.986380\n",
      "iteration 4400 / 5500: loss 3.784536\n",
      "iteration 4500 / 5500: loss 3.635188\n",
      "iteration 4600 / 5500: loss 3.497276\n",
      "iteration 4700 / 5500: loss 3.348325\n",
      "iteration 4800 / 5500: loss 3.203775\n",
      "iteration 4900 / 5500: loss 2.993415\n",
      "iteration 5000 / 5500: loss 2.932893\n",
      "iteration 5100 / 5500: loss 2.903004\n",
      "iteration 5200 / 5500: loss 2.749055\n",
      "iteration 5300 / 5500: loss 2.657801\n",
      "iteration 5400 / 5500: loss 2.599157\n",
      "lr 5.000000e-08 reg 1.000000e+04\n",
      "iteration 0 / 5500: loss 314.350025\n",
      "iteration 100 / 5500: loss 256.433324\n",
      "iteration 200 / 5500: loss 209.675819\n",
      "iteration 300 / 5500: loss 171.696744\n",
      "iteration 400 / 5500: loss 140.652965\n",
      "iteration 500 / 5500: loss 115.380326\n",
      "iteration 600 / 5500: loss 94.669350\n",
      "iteration 700 / 5500: loss 77.838533\n",
      "iteration 800 / 5500: loss 63.982746\n",
      "iteration 900 / 5500: loss 52.661030\n",
      "iteration 1000 / 5500: loss 43.478227\n",
      "iteration 1100 / 5500: loss 35.881984\n",
      "iteration 1200 / 5500: loss 29.758091\n",
      "iteration 1300 / 5500: loss 24.699437\n",
      "iteration 1400 / 5500: loss 20.506499\n",
      "iteration 1500 / 5500: loss 17.105606\n",
      "iteration 1600 / 5500: loss 14.366301\n",
      "iteration 1700 / 5500: loss 12.104294\n",
      "iteration 1800 / 5500: loss 10.336585\n",
      "iteration 1900 / 5500: loss 8.703895\n",
      "iteration 2000 / 5500: loss 7.535485\n",
      "iteration 2100 / 5500: loss 6.500193\n",
      "iteration 2200 / 5500: loss 5.681812\n",
      "iteration 2300 / 5500: loss 5.044890\n",
      "iteration 2400 / 5500: loss 4.403005\n",
      "iteration 2500 / 5500: loss 4.022678\n",
      "iteration 2600 / 5500: loss 3.666237\n",
      "iteration 2700 / 5500: loss 3.421908\n",
      "iteration 2800 / 5500: loss 3.131016\n",
      "iteration 2900 / 5500: loss 2.853776\n",
      "iteration 3000 / 5500: loss 2.808778\n",
      "iteration 3100 / 5500: loss 2.682871\n",
      "iteration 3200 / 5500: loss 2.538726\n",
      "iteration 3300 / 5500: loss 2.382005\n",
      "iteration 3400 / 5500: loss 2.323382\n",
      "iteration 3500 / 5500: loss 2.260275\n",
      "iteration 3600 / 5500: loss 2.217472\n",
      "iteration 3700 / 5500: loss 2.154015\n",
      "iteration 3800 / 5500: loss 2.036751\n",
      "iteration 3900 / 5500: loss 2.105409\n",
      "iteration 4000 / 5500: loss 2.103432\n",
      "iteration 4100 / 5500: loss 2.079227\n",
      "iteration 4200 / 5500: loss 2.091161\n",
      "iteration 4300 / 5500: loss 2.015772\n",
      "iteration 4400 / 5500: loss 2.066919\n",
      "iteration 4500 / 5500: loss 1.995508\n",
      "iteration 4600 / 5500: loss 2.060794\n",
      "iteration 4700 / 5500: loss 1.978795\n",
      "iteration 4800 / 5500: loss 2.048371\n",
      "iteration 4900 / 5500: loss 2.052759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5000 / 5500: loss 2.037053\n",
      "iteration 5100 / 5500: loss 2.019390\n",
      "iteration 5200 / 5500: loss 2.128438\n",
      "iteration 5300 / 5500: loss 2.040638\n",
      "iteration 5400 / 5500: loss 2.004338\n",
      "lr 5.000000e-08 reg 2.500000e+04\n",
      "iteration 0 / 5500: loss 778.552039\n",
      "iteration 100 / 5500: loss 471.789815\n",
      "iteration 200 / 5500: loss 286.131133\n",
      "iteration 300 / 5500: loss 174.017378\n",
      "iteration 400 / 5500: loss 106.143827\n",
      "iteration 500 / 5500: loss 64.995299\n",
      "iteration 600 / 5500: loss 40.160633\n",
      "iteration 700 / 5500: loss 25.132303\n",
      "iteration 800 / 5500: loss 16.036193\n",
      "iteration 900 / 5500: loss 10.464697\n",
      "iteration 1000 / 5500: loss 7.266627\n",
      "iteration 1100 / 5500: loss 5.187893\n",
      "iteration 1200 / 5500: loss 3.956010\n",
      "iteration 1300 / 5500: loss 3.210539\n",
      "iteration 1400 / 5500: loss 2.755768\n",
      "iteration 1500 / 5500: loss 2.455254\n",
      "iteration 1600 / 5500: loss 2.347772\n",
      "iteration 1700 / 5500: loss 2.189413\n",
      "iteration 1800 / 5500: loss 2.161550\n",
      "iteration 1900 / 5500: loss 2.146761\n",
      "iteration 2000 / 5500: loss 2.122761\n",
      "iteration 2100 / 5500: loss 2.024270\n",
      "iteration 2200 / 5500: loss 2.043309\n",
      "iteration 2300 / 5500: loss 2.064428\n",
      "iteration 2400 / 5500: loss 2.111472\n",
      "iteration 2500 / 5500: loss 2.104918\n",
      "iteration 2600 / 5500: loss 2.093354\n",
      "iteration 2700 / 5500: loss 2.072589\n",
      "iteration 2800 / 5500: loss 2.134032\n",
      "iteration 2900 / 5500: loss 2.133379\n",
      "iteration 3000 / 5500: loss 2.138352\n",
      "iteration 3100 / 5500: loss 2.101580\n",
      "iteration 3200 / 5500: loss 2.031874\n",
      "iteration 3300 / 5500: loss 2.058313\n",
      "iteration 3400 / 5500: loss 2.066119\n",
      "iteration 3500 / 5500: loss 2.115472\n",
      "iteration 3600 / 5500: loss 2.110422\n",
      "iteration 3700 / 5500: loss 2.098484\n",
      "iteration 3800 / 5500: loss 2.114405\n",
      "iteration 3900 / 5500: loss 2.089841\n",
      "iteration 4000 / 5500: loss 2.114977\n",
      "iteration 4100 / 5500: loss 2.060663\n",
      "iteration 4200 / 5500: loss 2.055063\n",
      "iteration 4300 / 5500: loss 2.039144\n",
      "iteration 4400 / 5500: loss 2.063435\n",
      "iteration 4500 / 5500: loss 2.038020\n",
      "iteration 4600 / 5500: loss 2.112682\n",
      "iteration 4700 / 5500: loss 2.100180\n",
      "iteration 4800 / 5500: loss 2.120451\n",
      "iteration 4900 / 5500: loss 2.039476\n",
      "iteration 5000 / 5500: loss 2.088590\n",
      "iteration 5100 / 5500: loss 2.108841\n",
      "iteration 5200 / 5500: loss 2.103532\n",
      "iteration 5300 / 5500: loss 2.055854\n",
      "iteration 5400 / 5500: loss 2.125285\n",
      "lr 5.000000e-08 reg 5.000000e+04\n",
      "iteration 0 / 5500: loss 1542.032971\n",
      "iteration 100 / 5500: loss 565.978308\n",
      "iteration 200 / 5500: loss 208.699315\n",
      "iteration 300 / 5500: loss 77.783182\n",
      "iteration 400 / 5500: loss 29.894450\n",
      "iteration 500 / 5500: loss 12.309198\n",
      "iteration 600 / 5500: loss 5.841907\n",
      "iteration 700 / 5500: loss 3.525737\n",
      "iteration 800 / 5500: loss 2.604746\n",
      "iteration 900 / 5500: loss 2.336856\n",
      "iteration 1000 / 5500: loss 2.209933\n",
      "iteration 1100 / 5500: loss 2.224785\n",
      "iteration 1200 / 5500: loss 2.145085\n",
      "iteration 1300 / 5500: loss 2.140489\n",
      "iteration 1400 / 5500: loss 2.144056\n",
      "iteration 1500 / 5500: loss 2.141936\n",
      "iteration 1600 / 5500: loss 2.103810\n",
      "iteration 1700 / 5500: loss 2.185367\n",
      "iteration 1800 / 5500: loss 2.106746\n",
      "iteration 1900 / 5500: loss 2.137787\n",
      "iteration 2000 / 5500: loss 2.176389\n",
      "iteration 2100 / 5500: loss 2.146498\n",
      "iteration 2200 / 5500: loss 2.172362\n",
      "iteration 2300 / 5500: loss 2.133075\n",
      "iteration 2400 / 5500: loss 2.129588\n",
      "iteration 2500 / 5500: loss 2.141670\n",
      "iteration 2600 / 5500: loss 2.144380\n",
      "iteration 2700 / 5500: loss 2.148133\n",
      "iteration 2800 / 5500: loss 2.149513\n",
      "iteration 2900 / 5500: loss 2.136112\n",
      "iteration 3000 / 5500: loss 2.136183\n",
      "iteration 3100 / 5500: loss 2.194680\n",
      "iteration 3200 / 5500: loss 2.088892\n",
      "iteration 3300 / 5500: loss 2.171731\n",
      "iteration 3400 / 5500: loss 2.169373\n",
      "iteration 3500 / 5500: loss 2.126743\n",
      "iteration 3600 / 5500: loss 2.171298\n",
      "iteration 3700 / 5500: loss 2.175009\n",
      "iteration 3800 / 5500: loss 2.138730\n",
      "iteration 3900 / 5500: loss 2.139335\n",
      "iteration 4000 / 5500: loss 2.138826\n",
      "iteration 4100 / 5500: loss 2.108996\n",
      "iteration 4200 / 5500: loss 2.130187\n",
      "iteration 4300 / 5500: loss 2.182791\n",
      "iteration 4400 / 5500: loss 2.198697\n",
      "iteration 4500 / 5500: loss 2.193691\n",
      "iteration 4600 / 5500: loss 2.139614\n",
      "iteration 4700 / 5500: loss 2.117876\n",
      "iteration 4800 / 5500: loss 2.157083\n",
      "iteration 4900 / 5500: loss 2.162164\n",
      "iteration 5000 / 5500: loss 2.129998\n",
      "iteration 5100 / 5500: loss 2.119369\n",
      "iteration 5200 / 5500: loss 2.147080\n",
      "iteration 5300 / 5500: loss 2.125950\n",
      "iteration 5400 / 5500: loss 2.136107\n",
      "lr 5.000000e-08 reg 1.000000e+05\n",
      "iteration 0 / 5500: loss 3054.608738\n",
      "iteration 100 / 5500: loss 410.316724\n",
      "iteration 200 / 5500: loss 56.791447\n",
      "iteration 300 / 5500: loss 9.478233\n",
      "iteration 400 / 5500: loss 3.151939\n",
      "iteration 500 / 5500: loss 2.316283\n",
      "iteration 600 / 5500: loss 2.220271\n",
      "iteration 700 / 5500: loss 2.189994\n",
      "iteration 800 / 5500: loss 2.243001\n",
      "iteration 900 / 5500: loss 2.232902\n",
      "iteration 1000 / 5500: loss 2.217757\n",
      "iteration 1100 / 5500: loss 2.157137\n",
      "iteration 1200 / 5500: loss 2.209118\n",
      "iteration 1300 / 5500: loss 2.160470\n",
      "iteration 1400 / 5500: loss 2.163431\n",
      "iteration 1500 / 5500: loss 2.157502\n",
      "iteration 1600 / 5500: loss 2.184061\n",
      "iteration 1700 / 5500: loss 2.205806\n",
      "iteration 1800 / 5500: loss 2.212737\n",
      "iteration 1900 / 5500: loss 2.165574\n",
      "iteration 2000 / 5500: loss 2.201122\n",
      "iteration 2100 / 5500: loss 2.184974\n",
      "iteration 2200 / 5500: loss 2.191970\n",
      "iteration 2300 / 5500: loss 2.197163\n",
      "iteration 2400 / 5500: loss 2.166376\n",
      "iteration 2500 / 5500: loss 2.174447\n",
      "iteration 2600 / 5500: loss 2.170008\n",
      "iteration 2700 / 5500: loss 2.191337\n",
      "iteration 2800 / 5500: loss 2.203355\n",
      "iteration 2900 / 5500: loss 2.203581\n",
      "iteration 3000 / 5500: loss 2.160756\n",
      "iteration 3100 / 5500: loss 2.212486\n",
      "iteration 3200 / 5500: loss 2.198350\n",
      "iteration 3300 / 5500: loss 2.201974\n",
      "iteration 3400 / 5500: loss 2.171857\n",
      "iteration 3500 / 5500: loss 2.194966\n",
      "iteration 3600 / 5500: loss 2.209103\n",
      "iteration 3700 / 5500: loss 2.204400\n",
      "iteration 3800 / 5500: loss 2.183538\n",
      "iteration 3900 / 5500: loss 2.167788\n",
      "iteration 4000 / 5500: loss 2.179150\n",
      "iteration 4100 / 5500: loss 2.200798\n",
      "iteration 4200 / 5500: loss 2.200484\n",
      "iteration 4300 / 5500: loss 2.195124\n",
      "iteration 4400 / 5500: loss 2.175320\n",
      "iteration 4500 / 5500: loss 2.176717\n",
      "iteration 4600 / 5500: loss 2.198157\n",
      "iteration 4700 / 5500: loss 2.170981\n",
      "iteration 4800 / 5500: loss 2.184838\n",
      "iteration 4900 / 5500: loss 2.227373\n",
      "iteration 5000 / 5500: loss 2.189298\n",
      "iteration 5100 / 5500: loss 2.191040\n",
      "iteration 5200 / 5500: loss 2.170358\n",
      "iteration 5300 / 5500: loss 2.151585\n",
      "iteration 5400 / 5500: loss 2.174569\n",
      "lr 5.000000e-08 reg 2.500000e+05\n",
      "iteration 0 / 5500: loss 7652.449518\n",
      "iteration 100 / 5500: loss 50.508937\n",
      "iteration 200 / 5500: loss 2.544129\n",
      "iteration 300 / 5500: loss 2.260525\n",
      "iteration 400 / 5500: loss 2.244677\n",
      "iteration 500 / 5500: loss 2.244746\n",
      "iteration 600 / 5500: loss 2.250108\n",
      "iteration 700 / 5500: loss 2.243867\n",
      "iteration 800 / 5500: loss 2.246167\n",
      "iteration 900 / 5500: loss 2.234769\n",
      "iteration 1000 / 5500: loss 2.236197\n",
      "iteration 1100 / 5500: loss 2.225024\n",
      "iteration 1200 / 5500: loss 2.252617\n",
      "iteration 1300 / 5500: loss 2.238469\n",
      "iteration 1400 / 5500: loss 2.246885\n",
      "iteration 1500 / 5500: loss 2.258153\n",
      "iteration 1600 / 5500: loss 2.238804\n",
      "iteration 1700 / 5500: loss 2.229165\n",
      "iteration 1800 / 5500: loss 2.257880\n",
      "iteration 1900 / 5500: loss 2.253421\n",
      "iteration 2000 / 5500: loss 2.255465\n",
      "iteration 2100 / 5500: loss 2.241510\n",
      "iteration 2200 / 5500: loss 2.254911\n",
      "iteration 2300 / 5500: loss 2.232506\n",
      "iteration 2400 / 5500: loss 2.227903\n",
      "iteration 2500 / 5500: loss 2.259844\n",
      "iteration 2600 / 5500: loss 2.245396\n",
      "iteration 2700 / 5500: loss 2.236564\n",
      "iteration 2800 / 5500: loss 2.240589\n",
      "iteration 2900 / 5500: loss 2.233351\n",
      "iteration 3000 / 5500: loss 2.250904\n",
      "iteration 3100 / 5500: loss 2.261187\n",
      "iteration 3200 / 5500: loss 2.255808\n",
      "iteration 3300 / 5500: loss 2.226747\n",
      "iteration 3400 / 5500: loss 2.247783\n",
      "iteration 3500 / 5500: loss 2.235782\n",
      "iteration 3600 / 5500: loss 2.231895\n",
      "iteration 3700 / 5500: loss 2.226854\n",
      "iteration 3800 / 5500: loss 2.246826\n",
      "iteration 3900 / 5500: loss 2.228828\n",
      "iteration 4000 / 5500: loss 2.240077\n",
      "iteration 4100 / 5500: loss 2.233074\n",
      "iteration 4200 / 5500: loss 2.243524\n",
      "iteration 4300 / 5500: loss 2.249120\n",
      "iteration 4400 / 5500: loss 2.245488\n",
      "iteration 4500 / 5500: loss 2.256720\n",
      "iteration 4600 / 5500: loss 2.239714\n",
      "iteration 4700 / 5500: loss 2.262246\n",
      "iteration 4800 / 5500: loss 2.228282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4900 / 5500: loss 2.243931\n",
      "iteration 5000 / 5500: loss 2.246246\n",
      "iteration 5100 / 5500: loss 2.256819\n",
      "iteration 5200 / 5500: loss 2.233150\n",
      "iteration 5300 / 5500: loss 2.232216\n",
      "iteration 5400 / 5500: loss 2.240272\n",
      "lr 1.000000e-07 reg 5.000000e+02\n",
      "iteration 0 / 5500: loss 21.287525\n",
      "iteration 100 / 5500: loss 19.592766\n",
      "iteration 200 / 5500: loss 18.772222\n",
      "iteration 300 / 5500: loss 18.076706\n",
      "iteration 400 / 5500: loss 17.579624\n",
      "iteration 500 / 5500: loss 16.876373\n",
      "iteration 600 / 5500: loss 16.741388\n",
      "iteration 700 / 5500: loss 16.269989\n",
      "iteration 800 / 5500: loss 15.694070\n",
      "iteration 900 / 5500: loss 15.529655\n",
      "iteration 1000 / 5500: loss 15.202878\n",
      "iteration 1100 / 5500: loss 14.701730\n",
      "iteration 1200 / 5500: loss 14.704534\n",
      "iteration 1300 / 5500: loss 14.385161\n",
      "iteration 1400 / 5500: loss 13.728199\n",
      "iteration 1500 / 5500: loss 13.806906\n",
      "iteration 1600 / 5500: loss 13.440516\n",
      "iteration 1700 / 5500: loss 13.125334\n",
      "iteration 1800 / 5500: loss 12.948763\n",
      "iteration 1900 / 5500: loss 12.600259\n",
      "iteration 2000 / 5500: loss 12.339964\n",
      "iteration 2100 / 5500: loss 12.179684\n",
      "iteration 2200 / 5500: loss 12.024786\n",
      "iteration 2300 / 5500: loss 12.019658\n",
      "iteration 2400 / 5500: loss 11.707649\n",
      "iteration 2500 / 5500: loss 11.297261\n",
      "iteration 2600 / 5500: loss 11.138359\n",
      "iteration 2700 / 5500: loss 10.867568\n",
      "iteration 2800 / 5500: loss 10.715072\n",
      "iteration 2900 / 5500: loss 10.345507\n",
      "iteration 3000 / 5500: loss 10.268006\n",
      "iteration 3100 / 5500: loss 10.167424\n",
      "iteration 3200 / 5500: loss 9.968935\n",
      "iteration 3300 / 5500: loss 10.041658\n",
      "iteration 3400 / 5500: loss 9.751602\n",
      "iteration 3500 / 5500: loss 9.486442\n",
      "iteration 3600 / 5500: loss 9.236453\n",
      "iteration 3700 / 5500: loss 9.266447\n",
      "iteration 3800 / 5500: loss 8.927214\n",
      "iteration 3900 / 5500: loss 8.764820\n",
      "iteration 4000 / 5500: loss 8.818071\n",
      "iteration 4100 / 5500: loss 8.567021\n",
      "iteration 4200 / 5500: loss 8.323549\n",
      "iteration 4300 / 5500: loss 8.208034\n",
      "iteration 4400 / 5500: loss 8.127065\n",
      "iteration 4500 / 5500: loss 7.839495\n",
      "iteration 4600 / 5500: loss 7.983402\n",
      "iteration 4700 / 5500: loss 7.832914\n",
      "iteration 4800 / 5500: loss 7.613900\n",
      "iteration 4900 / 5500: loss 7.387600\n",
      "iteration 5000 / 5500: loss 7.373694\n",
      "iteration 5100 / 5500: loss 7.265802\n",
      "iteration 5200 / 5500: loss 7.249588\n",
      "iteration 5300 / 5500: loss 7.080023\n",
      "iteration 5400 / 5500: loss 7.024472\n",
      "lr 1.000000e-07 reg 1.000000e+03\n",
      "iteration 0 / 5500: loss 36.981125\n",
      "iteration 100 / 5500: loss 33.179411\n",
      "iteration 200 / 5500: loss 31.500000\n",
      "iteration 300 / 5500: loss 30.050997\n",
      "iteration 400 / 5500: loss 28.666040\n",
      "iteration 500 / 5500: loss 27.893370\n",
      "iteration 600 / 5500: loss 26.715852\n",
      "iteration 700 / 5500: loss 25.381871\n",
      "iteration 800 / 5500: loss 24.454334\n",
      "iteration 900 / 5500: loss 23.543463\n",
      "iteration 1000 / 5500: loss 22.765360\n",
      "iteration 1100 / 5500: loss 21.528212\n",
      "iteration 1200 / 5500: loss 20.773578\n",
      "iteration 1300 / 5500: loss 20.101417\n",
      "iteration 1400 / 5500: loss 19.349327\n",
      "iteration 1500 / 5500: loss 18.722595\n",
      "iteration 1600 / 5500: loss 17.962473\n",
      "iteration 1700 / 5500: loss 17.362414\n",
      "iteration 1800 / 5500: loss 16.670368\n",
      "iteration 1900 / 5500: loss 15.846236\n",
      "iteration 2000 / 5500: loss 15.501949\n",
      "iteration 2100 / 5500: loss 14.946032\n",
      "iteration 2200 / 5500: loss 14.369924\n",
      "iteration 2300 / 5500: loss 13.855814\n",
      "iteration 2400 / 5500: loss 13.380639\n",
      "iteration 2500 / 5500: loss 12.874053\n",
      "iteration 2600 / 5500: loss 12.398136\n",
      "iteration 2700 / 5500: loss 12.108481\n",
      "iteration 2800 / 5500: loss 11.706367\n",
      "iteration 2900 / 5500: loss 11.254972\n",
      "iteration 3000 / 5500: loss 10.853455\n",
      "iteration 3100 / 5500: loss 10.544305\n",
      "iteration 3200 / 5500: loss 10.098448\n",
      "iteration 3300 / 5500: loss 9.692415\n",
      "iteration 3400 / 5500: loss 9.416866\n",
      "iteration 3500 / 5500: loss 9.138319\n",
      "iteration 3600 / 5500: loss 8.845430\n",
      "iteration 3700 / 5500: loss 8.505232\n",
      "iteration 3800 / 5500: loss 8.215819\n",
      "iteration 3900 / 5500: loss 8.052989\n",
      "iteration 4000 / 5500: loss 7.818233\n",
      "iteration 4100 / 5500: loss 7.620134\n",
      "iteration 4200 / 5500: loss 7.346718\n",
      "iteration 4300 / 5500: loss 7.095740\n",
      "iteration 4400 / 5500: loss 6.844710\n",
      "iteration 4500 / 5500: loss 6.718687\n",
      "iteration 4600 / 5500: loss 6.356186\n",
      "iteration 4700 / 5500: loss 6.412254\n",
      "iteration 4800 / 5500: loss 6.211554\n",
      "iteration 4900 / 5500: loss 5.964854\n",
      "iteration 5000 / 5500: loss 5.900911\n",
      "iteration 5100 / 5500: loss 5.599011\n",
      "iteration 5200 / 5500: loss 5.481939\n",
      "iteration 5300 / 5500: loss 5.302272\n",
      "iteration 5400 / 5500: loss 5.133584\n",
      "lr 1.000000e-07 reg 5.000000e+03\n",
      "iteration 0 / 5500: loss 160.194201\n",
      "iteration 100 / 5500: loss 129.719437\n",
      "iteration 200 / 5500: loss 106.013613\n",
      "iteration 300 / 5500: loss 86.814000\n",
      "iteration 400 / 5500: loss 71.373205\n",
      "iteration 500 / 5500: loss 58.634052\n",
      "iteration 600 / 5500: loss 48.206054\n",
      "iteration 700 / 5500: loss 39.818537\n",
      "iteration 800 / 5500: loss 32.977496\n",
      "iteration 900 / 5500: loss 27.133791\n",
      "iteration 1000 / 5500: loss 22.722395\n",
      "iteration 1100 / 5500: loss 18.940901\n",
      "iteration 1200 / 5500: loss 15.745069\n",
      "iteration 1300 / 5500: loss 13.292126\n",
      "iteration 1400 / 5500: loss 11.155743\n",
      "iteration 1500 / 5500: loss 9.468583\n",
      "iteration 1600 / 5500: loss 8.127542\n",
      "iteration 1700 / 5500: loss 6.957952\n",
      "iteration 1800 / 5500: loss 5.966865\n",
      "iteration 1900 / 5500: loss 5.285187\n",
      "iteration 2000 / 5500: loss 4.750980\n",
      "iteration 2100 / 5500: loss 4.236523\n",
      "iteration 2200 / 5500: loss 3.784094\n",
      "iteration 2300 / 5500: loss 3.425817\n",
      "iteration 2400 / 5500: loss 3.117301\n",
      "iteration 2500 / 5500: loss 2.922722\n",
      "iteration 2600 / 5500: loss 2.810055\n",
      "iteration 2700 / 5500: loss 2.671081\n",
      "iteration 2800 / 5500: loss 2.540855\n",
      "iteration 2900 / 5500: loss 2.322242\n",
      "iteration 3000 / 5500: loss 2.341981\n",
      "iteration 3100 / 5500: loss 2.189494\n",
      "iteration 3200 / 5500: loss 2.284685\n",
      "iteration 3300 / 5500: loss 2.075297\n",
      "iteration 3400 / 5500: loss 2.107866\n",
      "iteration 3500 / 5500: loss 2.040555\n",
      "iteration 3600 / 5500: loss 2.110989\n",
      "iteration 3700 / 5500: loss 2.092521\n",
      "iteration 3800 / 5500: loss 1.991750\n",
      "iteration 3900 / 5500: loss 2.045661\n",
      "iteration 4000 / 5500: loss 1.998490\n",
      "iteration 4100 / 5500: loss 2.001683\n",
      "iteration 4200 / 5500: loss 2.009570\n",
      "iteration 4300 / 5500: loss 1.900918\n",
      "iteration 4400 / 5500: loss 1.889502\n",
      "iteration 4500 / 5500: loss 1.870151\n",
      "iteration 4600 / 5500: loss 1.982414\n",
      "iteration 4700 / 5500: loss 1.972726\n",
      "iteration 4800 / 5500: loss 1.955059\n",
      "iteration 4900 / 5500: loss 1.967486\n",
      "iteration 5000 / 5500: loss 1.894925\n",
      "iteration 5100 / 5500: loss 1.919859\n",
      "iteration 5200 / 5500: loss 1.930502\n",
      "iteration 5300 / 5500: loss 1.931103\n",
      "iteration 5400 / 5500: loss 2.004315\n",
      "lr 1.000000e-07 reg 1.000000e+04\n",
      "iteration 0 / 5500: loss 312.182010\n",
      "iteration 100 / 5500: loss 208.151318\n",
      "iteration 200 / 5500: loss 139.726232\n",
      "iteration 300 / 5500: loss 94.035969\n",
      "iteration 400 / 5500: loss 63.522671\n",
      "iteration 500 / 5500: loss 43.202669\n",
      "iteration 600 / 5500: loss 29.509693\n",
      "iteration 700 / 5500: loss 20.367601\n",
      "iteration 800 / 5500: loss 14.252778\n",
      "iteration 900 / 5500: loss 10.295009\n",
      "iteration 1000 / 5500: loss 7.521081\n",
      "iteration 1100 / 5500: loss 5.699609\n",
      "iteration 1200 / 5500: loss 4.511301\n",
      "iteration 1300 / 5500: loss 3.608533\n",
      "iteration 1400 / 5500: loss 3.146197\n",
      "iteration 1500 / 5500: loss 2.873256\n",
      "iteration 1600 / 5500: loss 2.549633\n",
      "iteration 1700 / 5500: loss 2.353865\n",
      "iteration 1800 / 5500: loss 2.244430\n",
      "iteration 1900 / 5500: loss 2.085802\n",
      "iteration 2000 / 5500: loss 2.085035\n",
      "iteration 2100 / 5500: loss 2.040479\n",
      "iteration 2200 / 5500: loss 2.136216\n",
      "iteration 2300 / 5500: loss 2.038608\n",
      "iteration 2400 / 5500: loss 2.042314\n",
      "iteration 2500 / 5500: loss 2.015387\n",
      "iteration 2600 / 5500: loss 2.057457\n",
      "iteration 2700 / 5500: loss 2.100568\n",
      "iteration 2800 / 5500: loss 2.104553\n",
      "iteration 2900 / 5500: loss 2.020067\n",
      "iteration 3000 / 5500: loss 2.020405\n",
      "iteration 3100 / 5500: loss 2.022482\n",
      "iteration 3200 / 5500: loss 1.972296\n",
      "iteration 3300 / 5500: loss 2.123952\n",
      "iteration 3400 / 5500: loss 1.988736\n",
      "iteration 3500 / 5500: loss 2.031135\n",
      "iteration 3600 / 5500: loss 2.007171\n",
      "iteration 3700 / 5500: loss 1.947683\n",
      "iteration 3800 / 5500: loss 2.095280\n",
      "iteration 3900 / 5500: loss 1.942809\n",
      "iteration 4000 / 5500: loss 2.066116\n",
      "iteration 4100 / 5500: loss 2.000559\n",
      "iteration 4200 / 5500: loss 2.010238\n",
      "iteration 4300 / 5500: loss 2.003364\n",
      "iteration 4400 / 5500: loss 2.024570\n",
      "iteration 4500 / 5500: loss 2.051594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4600 / 5500: loss 2.058227\n",
      "iteration 4700 / 5500: loss 2.016686\n",
      "iteration 4800 / 5500: loss 2.046883\n",
      "iteration 4900 / 5500: loss 2.029260\n",
      "iteration 5000 / 5500: loss 1.937771\n",
      "iteration 5100 / 5500: loss 2.074977\n",
      "iteration 5200 / 5500: loss 1.936348\n",
      "iteration 5300 / 5500: loss 1.920069\n",
      "iteration 5400 / 5500: loss 2.046177\n",
      "lr 1.000000e-07 reg 2.500000e+04\n",
      "iteration 0 / 5500: loss 763.020191\n",
      "iteration 100 / 5500: loss 279.773712\n",
      "iteration 200 / 5500: loss 103.631457\n",
      "iteration 300 / 5500: loss 39.302178\n",
      "iteration 400 / 5500: loss 15.720967\n",
      "iteration 500 / 5500: loss 7.005355\n",
      "iteration 600 / 5500: loss 3.904799\n",
      "iteration 700 / 5500: loss 2.803936\n",
      "iteration 800 / 5500: loss 2.324657\n",
      "iteration 900 / 5500: loss 2.160812\n",
      "iteration 1000 / 5500: loss 2.062032\n",
      "iteration 1100 / 5500: loss 2.131260\n",
      "iteration 1200 / 5500: loss 2.046211\n",
      "iteration 1300 / 5500: loss 2.107473\n",
      "iteration 1400 / 5500: loss 2.069964\n",
      "iteration 1500 / 5500: loss 2.083030\n",
      "iteration 1600 / 5500: loss 2.066984\n",
      "iteration 1700 / 5500: loss 2.012551\n",
      "iteration 1800 / 5500: loss 2.063643\n",
      "iteration 1900 / 5500: loss 2.078634\n",
      "iteration 2000 / 5500: loss 2.044087\n",
      "iteration 2100 / 5500: loss 2.043666\n",
      "iteration 2200 / 5500: loss 2.021080\n",
      "iteration 2300 / 5500: loss 2.032980\n",
      "iteration 2400 / 5500: loss 2.055771\n",
      "iteration 2500 / 5500: loss 2.060575\n",
      "iteration 2600 / 5500: loss 2.065280\n",
      "iteration 2700 / 5500: loss 2.062658\n",
      "iteration 2800 / 5500: loss 2.111943\n",
      "iteration 2900 / 5500: loss 2.104210\n",
      "iteration 3000 / 5500: loss 2.089005\n",
      "iteration 3100 / 5500: loss 2.037886\n",
      "iteration 3200 / 5500: loss 2.136168\n",
      "iteration 3300 / 5500: loss 2.056292\n",
      "iteration 3400 / 5500: loss 2.081497\n",
      "iteration 3500 / 5500: loss 2.114097\n",
      "iteration 3600 / 5500: loss 2.089402\n",
      "iteration 3700 / 5500: loss 2.124772\n",
      "iteration 3800 / 5500: loss 2.140851\n",
      "iteration 3900 / 5500: loss 2.010200\n",
      "iteration 4000 / 5500: loss 2.045711\n",
      "iteration 4100 / 5500: loss 2.031385\n",
      "iteration 4200 / 5500: loss 2.131003\n",
      "iteration 4300 / 5500: loss 2.070208\n",
      "iteration 4400 / 5500: loss 2.096812\n",
      "iteration 4500 / 5500: loss 2.088670\n",
      "iteration 4600 / 5500: loss 2.100782\n",
      "iteration 4700 / 5500: loss 2.087721\n",
      "iteration 4800 / 5500: loss 2.086480\n",
      "iteration 4900 / 5500: loss 2.077767\n",
      "iteration 5000 / 5500: loss 2.087114\n",
      "iteration 5100 / 5500: loss 2.133951\n",
      "iteration 5200 / 5500: loss 2.066473\n",
      "iteration 5300 / 5500: loss 2.047464\n",
      "iteration 5400 / 5500: loss 2.067163\n",
      "lr 1.000000e-07 reg 5.000000e+04\n",
      "iteration 0 / 5500: loss 1547.984926\n",
      "iteration 100 / 5500: loss 208.506090\n",
      "iteration 200 / 5500: loss 29.697846\n",
      "iteration 300 / 5500: loss 5.818633\n",
      "iteration 400 / 5500: loss 2.647651\n",
      "iteration 500 / 5500: loss 2.195863\n",
      "iteration 600 / 5500: loss 2.110234\n",
      "iteration 700 / 5500: loss 2.153893\n",
      "iteration 800 / 5500: loss 2.063529\n",
      "iteration 900 / 5500: loss 2.105249\n",
      "iteration 1000 / 5500: loss 2.130162\n",
      "iteration 1100 / 5500: loss 2.154102\n",
      "iteration 1200 / 5500: loss 2.160833\n",
      "iteration 1300 / 5500: loss 2.133599\n",
      "iteration 1400 / 5500: loss 2.161434\n",
      "iteration 1500 / 5500: loss 2.162241\n",
      "iteration 1600 / 5500: loss 2.108716\n",
      "iteration 1700 / 5500: loss 2.179679\n",
      "iteration 1800 / 5500: loss 2.140300\n",
      "iteration 1900 / 5500: loss 2.156689\n",
      "iteration 2000 / 5500: loss 2.141342\n",
      "iteration 2100 / 5500: loss 2.148896\n",
      "iteration 2200 / 5500: loss 2.173473\n",
      "iteration 2300 / 5500: loss 2.104810\n",
      "iteration 2400 / 5500: loss 2.171603\n",
      "iteration 2500 / 5500: loss 2.140190\n",
      "iteration 2600 / 5500: loss 2.124690\n",
      "iteration 2700 / 5500: loss 2.135440\n",
      "iteration 2800 / 5500: loss 2.137217\n",
      "iteration 2900 / 5500: loss 2.180273\n",
      "iteration 3000 / 5500: loss 2.145280\n",
      "iteration 3100 / 5500: loss 2.132725\n",
      "iteration 3200 / 5500: loss 2.124983\n",
      "iteration 3300 / 5500: loss 2.093676\n",
      "iteration 3400 / 5500: loss 2.170163\n",
      "iteration 3500 / 5500: loss 2.120067\n",
      "iteration 3600 / 5500: loss 2.093493\n",
      "iteration 3700 / 5500: loss 2.169127\n",
      "iteration 3800 / 5500: loss 2.107581\n",
      "iteration 3900 / 5500: loss 2.174435\n",
      "iteration 4000 / 5500: loss 2.127034\n",
      "iteration 4100 / 5500: loss 2.177559\n",
      "iteration 4200 / 5500: loss 2.148024\n",
      "iteration 4300 / 5500: loss 2.121571\n",
      "iteration 4400 / 5500: loss 2.151831\n",
      "iteration 4500 / 5500: loss 2.101877\n",
      "iteration 4600 / 5500: loss 2.110942\n",
      "iteration 4700 / 5500: loss 2.160908\n",
      "iteration 4800 / 5500: loss 2.125783\n",
      "iteration 4900 / 5500: loss 2.169843\n",
      "iteration 5000 / 5500: loss 2.181824\n",
      "iteration 5100 / 5500: loss 2.140624\n",
      "iteration 5200 / 5500: loss 2.126918\n",
      "iteration 5300 / 5500: loss 2.103482\n",
      "iteration 5400 / 5500: loss 2.142876\n",
      "lr 1.000000e-07 reg 1.000000e+05\n",
      "iteration 0 / 5500: loss 3086.246780\n",
      "iteration 100 / 5500: loss 56.142394\n",
      "iteration 200 / 5500: loss 3.157476\n",
      "iteration 300 / 5500: loss 2.220017\n",
      "iteration 400 / 5500: loss 2.242680\n",
      "iteration 500 / 5500: loss 2.182319\n",
      "iteration 600 / 5500: loss 2.210071\n",
      "iteration 700 / 5500: loss 2.181055\n",
      "iteration 800 / 5500: loss 2.196398\n",
      "iteration 900 / 5500: loss 2.218385\n",
      "iteration 1000 / 5500: loss 2.201115\n",
      "iteration 1100 / 5500: loss 2.183677\n",
      "iteration 1200 / 5500: loss 2.227288\n",
      "iteration 1300 / 5500: loss 2.182025\n",
      "iteration 1400 / 5500: loss 2.222787\n",
      "iteration 1500 / 5500: loss 2.209005\n",
      "iteration 1600 / 5500: loss 2.185255\n",
      "iteration 1700 / 5500: loss 2.178088\n",
      "iteration 1800 / 5500: loss 2.193191\n",
      "iteration 1900 / 5500: loss 2.192037\n",
      "iteration 2000 / 5500: loss 2.170334\n",
      "iteration 2100 / 5500: loss 2.208222\n",
      "iteration 2200 / 5500: loss 2.191533\n",
      "iteration 2300 / 5500: loss 2.242865\n",
      "iteration 2400 / 5500: loss 2.191313\n",
      "iteration 2500 / 5500: loss 2.216484\n",
      "iteration 2600 / 5500: loss 2.199075\n",
      "iteration 2700 / 5500: loss 2.208951\n",
      "iteration 2800 / 5500: loss 2.215711\n",
      "iteration 2900 / 5500: loss 2.211259\n",
      "iteration 3000 / 5500: loss 2.206647\n",
      "iteration 3100 / 5500: loss 2.202303\n",
      "iteration 3200 / 5500: loss 2.176438\n",
      "iteration 3300 / 5500: loss 2.188109\n",
      "iteration 3400 / 5500: loss 2.211707\n",
      "iteration 3500 / 5500: loss 2.185379\n",
      "iteration 3600 / 5500: loss 2.164166\n",
      "iteration 3700 / 5500: loss 2.171548\n",
      "iteration 3800 / 5500: loss 2.202983\n",
      "iteration 3900 / 5500: loss 2.198327\n",
      "iteration 4000 / 5500: loss 2.215023\n",
      "iteration 4100 / 5500: loss 2.188243\n",
      "iteration 4200 / 5500: loss 2.180508\n",
      "iteration 4300 / 5500: loss 2.220683\n",
      "iteration 4400 / 5500: loss 2.218086\n",
      "iteration 4500 / 5500: loss 2.179476\n",
      "iteration 4600 / 5500: loss 2.197696\n",
      "iteration 4700 / 5500: loss 2.222639\n",
      "iteration 4800 / 5500: loss 2.152254\n",
      "iteration 4900 / 5500: loss 2.218777\n",
      "iteration 5000 / 5500: loss 2.213857\n",
      "iteration 5100 / 5500: loss 2.220015\n",
      "iteration 5200 / 5500: loss 2.184452\n",
      "iteration 5300 / 5500: loss 2.178538\n",
      "iteration 5400 / 5500: loss 2.205843\n",
      "lr 1.000000e-07 reg 2.500000e+05\n",
      "iteration 0 / 5500: loss 7617.090971\n",
      "iteration 100 / 5500: loss 2.521351\n",
      "iteration 200 / 5500: loss 2.234214\n",
      "iteration 300 / 5500: loss 2.242896\n",
      "iteration 400 / 5500: loss 2.271701\n",
      "iteration 500 / 5500: loss 2.240583\n",
      "iteration 600 / 5500: loss 2.224961\n",
      "iteration 700 / 5500: loss 2.268388\n",
      "iteration 800 / 5500: loss 2.217546\n",
      "iteration 900 / 5500: loss 2.230349\n",
      "iteration 1000 / 5500: loss 2.243371\n",
      "iteration 1100 / 5500: loss 2.233428\n",
      "iteration 1200 / 5500: loss 2.264220\n",
      "iteration 1300 / 5500: loss 2.242690\n",
      "iteration 1400 / 5500: loss 2.249608\n",
      "iteration 1500 / 5500: loss 2.242386\n",
      "iteration 1600 / 5500: loss 2.262391\n",
      "iteration 1700 / 5500: loss 2.255734\n",
      "iteration 1800 / 5500: loss 2.245562\n",
      "iteration 1900 / 5500: loss 2.248167\n",
      "iteration 2000 / 5500: loss 2.247376\n",
      "iteration 2100 / 5500: loss 2.252898\n",
      "iteration 2200 / 5500: loss 2.244517\n",
      "iteration 2300 / 5500: loss 2.242748\n",
      "iteration 2400 / 5500: loss 2.244752\n",
      "iteration 2500 / 5500: loss 2.222130\n",
      "iteration 2600 / 5500: loss 2.234116\n",
      "iteration 2700 / 5500: loss 2.253816\n",
      "iteration 2800 / 5500: loss 2.247052\n",
      "iteration 2900 / 5500: loss 2.236818\n",
      "iteration 3000 / 5500: loss 2.247730\n",
      "iteration 3100 / 5500: loss 2.235721\n",
      "iteration 3200 / 5500: loss 2.247531\n",
      "iteration 3300 / 5500: loss 2.246822\n",
      "iteration 3400 / 5500: loss 2.231964\n",
      "iteration 3500 / 5500: loss 2.253415\n",
      "iteration 3600 / 5500: loss 2.251482\n",
      "iteration 3700 / 5500: loss 2.228502\n",
      "iteration 3800 / 5500: loss 2.241503\n",
      "iteration 3900 / 5500: loss 2.258419\n",
      "iteration 4000 / 5500: loss 2.253224\n",
      "iteration 4100 / 5500: loss 2.228334\n",
      "iteration 4200 / 5500: loss 2.237912\n",
      "iteration 4300 / 5500: loss 2.236476\n",
      "iteration 4400 / 5500: loss 2.257183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4500 / 5500: loss 2.248988\n",
      "iteration 4600 / 5500: loss 2.220454\n",
      "iteration 4700 / 5500: loss 2.246884\n",
      "iteration 4800 / 5500: loss 2.267285\n",
      "iteration 4900 / 5500: loss 2.232721\n",
      "iteration 5000 / 5500: loss 2.236659\n",
      "iteration 5100 / 5500: loss 2.225170\n",
      "iteration 5200 / 5500: loss 2.237449\n",
      "iteration 5300 / 5500: loss 2.253879\n",
      "iteration 5400 / 5500: loss 2.240262\n",
      "lr 5.000000e-07 reg 5.000000e+02\n",
      "iteration 0 / 5500: loss 20.951044\n",
      "iteration 100 / 5500: loss 16.832257\n",
      "iteration 200 / 5500: loss 14.886079\n",
      "iteration 300 / 5500: loss 13.528351\n",
      "iteration 400 / 5500: loss 12.383899\n",
      "iteration 500 / 5500: loss 11.246739\n",
      "iteration 600 / 5500: loss 10.238914\n",
      "iteration 700 / 5500: loss 9.518243\n",
      "iteration 800 / 5500: loss 8.675876\n",
      "iteration 900 / 5500: loss 7.962636\n",
      "iteration 1000 / 5500: loss 7.288368\n",
      "iteration 1100 / 5500: loss 6.857211\n",
      "iteration 1200 / 5500: loss 6.246734\n",
      "iteration 1300 / 5500: loss 5.931689\n",
      "iteration 1400 / 5500: loss 5.490069\n",
      "iteration 1500 / 5500: loss 5.085428\n",
      "iteration 1600 / 5500: loss 4.678685\n",
      "iteration 1700 / 5500: loss 4.464031\n",
      "iteration 1800 / 5500: loss 4.194697\n",
      "iteration 1900 / 5500: loss 4.015615\n",
      "iteration 2000 / 5500: loss 3.817223\n",
      "iteration 2100 / 5500: loss 3.484386\n",
      "iteration 2200 / 5500: loss 3.367103\n",
      "iteration 2300 / 5500: loss 3.244947\n",
      "iteration 2400 / 5500: loss 3.032758\n",
      "iteration 2500 / 5500: loss 2.997719\n",
      "iteration 2600 / 5500: loss 2.903384\n",
      "iteration 2700 / 5500: loss 2.676617\n",
      "iteration 2800 / 5500: loss 2.630375\n",
      "iteration 2900 / 5500: loss 2.542566\n",
      "iteration 3000 / 5500: loss 2.518798\n",
      "iteration 3100 / 5500: loss 2.348805\n",
      "iteration 3200 / 5500: loss 2.448583\n",
      "iteration 3300 / 5500: loss 2.189627\n",
      "iteration 3400 / 5500: loss 2.301497\n",
      "iteration 3500 / 5500: loss 2.180634\n",
      "iteration 3600 / 5500: loss 2.232907\n",
      "iteration 3700 / 5500: loss 2.211878\n",
      "iteration 3800 / 5500: loss 2.013702\n",
      "iteration 3900 / 5500: loss 2.012782\n",
      "iteration 4000 / 5500: loss 2.037295\n",
      "iteration 4100 / 5500: loss 2.098279\n",
      "iteration 4200 / 5500: loss 1.911726\n",
      "iteration 4300 / 5500: loss 2.095264\n",
      "iteration 4400 / 5500: loss 1.899700\n",
      "iteration 4500 / 5500: loss 1.998967\n",
      "iteration 4600 / 5500: loss 1.938565\n",
      "iteration 4700 / 5500: loss 2.026960\n",
      "iteration 4800 / 5500: loss 1.913967\n",
      "iteration 4900 / 5500: loss 1.806940\n",
      "iteration 5000 / 5500: loss 1.999627\n",
      "iteration 5100 / 5500: loss 1.708831\n",
      "iteration 5200 / 5500: loss 1.832598\n",
      "iteration 5300 / 5500: loss 1.814570\n",
      "iteration 5400 / 5500: loss 1.788270\n",
      "lr 5.000000e-07 reg 1.000000e+03\n",
      "iteration 0 / 5500: loss 35.376444\n",
      "iteration 100 / 5500: loss 27.763844\n",
      "iteration 200 / 5500: loss 22.734930\n",
      "iteration 300 / 5500: loss 18.842389\n",
      "iteration 400 / 5500: loss 15.554385\n",
      "iteration 500 / 5500: loss 12.892727\n",
      "iteration 600 / 5500: loss 10.852497\n",
      "iteration 700 / 5500: loss 9.136921\n",
      "iteration 800 / 5500: loss 7.931470\n",
      "iteration 900 / 5500: loss 6.681566\n",
      "iteration 1000 / 5500: loss 5.834588\n",
      "iteration 1100 / 5500: loss 5.040506\n",
      "iteration 1200 / 5500: loss 4.556882\n",
      "iteration 1300 / 5500: loss 3.962227\n",
      "iteration 1400 / 5500: loss 3.569918\n",
      "iteration 1500 / 5500: loss 3.304389\n",
      "iteration 1600 / 5500: loss 3.096956\n",
      "iteration 1700 / 5500: loss 2.698892\n",
      "iteration 1800 / 5500: loss 2.716479\n",
      "iteration 1900 / 5500: loss 2.494633\n",
      "iteration 2000 / 5500: loss 2.370225\n",
      "iteration 2100 / 5500: loss 2.224759\n",
      "iteration 2200 / 5500: loss 2.161584\n",
      "iteration 2300 / 5500: loss 2.095476\n",
      "iteration 2400 / 5500: loss 2.092737\n",
      "iteration 2500 / 5500: loss 1.981946\n",
      "iteration 2600 / 5500: loss 1.949443\n",
      "iteration 2700 / 5500: loss 1.947429\n",
      "iteration 2800 / 5500: loss 1.948813\n",
      "iteration 2900 / 5500: loss 1.924327\n",
      "iteration 3000 / 5500: loss 1.891654\n",
      "iteration 3100 / 5500: loss 1.877116\n",
      "iteration 3200 / 5500: loss 1.909528\n",
      "iteration 3300 / 5500: loss 1.957048\n",
      "iteration 3400 / 5500: loss 1.859619\n",
      "iteration 3500 / 5500: loss 1.834751\n",
      "iteration 3600 / 5500: loss 1.824322\n",
      "iteration 3700 / 5500: loss 1.766113\n",
      "iteration 3800 / 5500: loss 1.828042\n",
      "iteration 3900 / 5500: loss 1.863890\n",
      "iteration 4000 / 5500: loss 1.833241\n",
      "iteration 4100 / 5500: loss 1.799248\n",
      "iteration 4200 / 5500: loss 1.846657\n",
      "iteration 4300 / 5500: loss 1.810032\n",
      "iteration 4400 / 5500: loss 1.931867\n",
      "iteration 4500 / 5500: loss 1.877980\n",
      "iteration 4600 / 5500: loss 1.890434\n",
      "iteration 4700 / 5500: loss 1.735486\n",
      "iteration 4800 / 5500: loss 1.812063\n",
      "iteration 4900 / 5500: loss 1.961349\n",
      "iteration 5000 / 5500: loss 1.910077\n",
      "iteration 5100 / 5500: loss 1.748625\n",
      "iteration 5200 / 5500: loss 1.851395\n",
      "iteration 5300 / 5500: loss 1.883576\n",
      "iteration 5400 / 5500: loss 1.875303\n",
      "lr 5.000000e-07 reg 5.000000e+03\n",
      "iteration 0 / 5500: loss 158.388407\n",
      "iteration 100 / 5500: loss 58.036055\n",
      "iteration 200 / 5500: loss 22.323856\n",
      "iteration 300 / 5500: loss 9.425765\n",
      "iteration 400 / 5500: loss 4.623052\n",
      "iteration 500 / 5500: loss 2.893135\n",
      "iteration 600 / 5500: loss 2.317642\n",
      "iteration 700 / 5500: loss 2.092981\n",
      "iteration 800 / 5500: loss 2.016500\n",
      "iteration 900 / 5500: loss 1.890387\n",
      "iteration 1000 / 5500: loss 1.925035\n",
      "iteration 1100 / 5500: loss 1.992298\n",
      "iteration 1200 / 5500: loss 1.942207\n",
      "iteration 1300 / 5500: loss 1.956116\n",
      "iteration 1400 / 5500: loss 1.947362\n",
      "iteration 1500 / 5500: loss 1.904983\n",
      "iteration 1600 / 5500: loss 1.937386\n",
      "iteration 1700 / 5500: loss 1.923632\n",
      "iteration 1800 / 5500: loss 1.929614\n",
      "iteration 1900 / 5500: loss 1.968949\n",
      "iteration 2000 / 5500: loss 1.969299\n",
      "iteration 2100 / 5500: loss 1.951072\n",
      "iteration 2200 / 5500: loss 1.909757\n",
      "iteration 2300 / 5500: loss 1.861153\n",
      "iteration 2400 / 5500: loss 1.890908\n",
      "iteration 2500 / 5500: loss 1.951294\n",
      "iteration 2600 / 5500: loss 1.966525\n",
      "iteration 2700 / 5500: loss 1.952370\n",
      "iteration 2800 / 5500: loss 1.893341\n",
      "iteration 2900 / 5500: loss 1.923547\n",
      "iteration 3000 / 5500: loss 1.981980\n",
      "iteration 3100 / 5500: loss 1.931490\n",
      "iteration 3200 / 5500: loss 1.959055\n",
      "iteration 3300 / 5500: loss 1.957145\n",
      "iteration 3400 / 5500: loss 1.969892\n",
      "iteration 3500 / 5500: loss 1.929833\n",
      "iteration 3600 / 5500: loss 1.872183\n",
      "iteration 3700 / 5500: loss 1.996728\n",
      "iteration 3800 / 5500: loss 1.972304\n",
      "iteration 3900 / 5500: loss 1.872753\n",
      "iteration 4000 / 5500: loss 1.938521\n",
      "iteration 4100 / 5500: loss 1.936272\n",
      "iteration 4200 / 5500: loss 1.962893\n",
      "iteration 4300 / 5500: loss 1.985767\n",
      "iteration 4400 / 5500: loss 1.936742\n",
      "iteration 4500 / 5500: loss 1.912456\n",
      "iteration 4600 / 5500: loss 1.947266\n",
      "iteration 4700 / 5500: loss 1.931321\n",
      "iteration 4800 / 5500: loss 1.996728\n",
      "iteration 4900 / 5500: loss 1.910660\n",
      "iteration 5000 / 5500: loss 1.935243\n",
      "iteration 5100 / 5500: loss 1.977243\n",
      "iteration 5200 / 5500: loss 1.865800\n",
      "iteration 5300 / 5500: loss 1.999098\n",
      "iteration 5400 / 5500: loss 1.946728\n",
      "lr 5.000000e-07 reg 1.000000e+04\n",
      "iteration 0 / 5500: loss 315.520819\n",
      "iteration 100 / 5500: loss 43.261104\n",
      "iteration 200 / 5500: loss 7.500001\n",
      "iteration 300 / 5500: loss 2.768410\n",
      "iteration 400 / 5500: loss 2.047684\n",
      "iteration 500 / 5500: loss 2.003808\n",
      "iteration 600 / 5500: loss 1.969443\n",
      "iteration 700 / 5500: loss 1.995606\n",
      "iteration 800 / 5500: loss 2.042761\n",
      "iteration 900 / 5500: loss 2.001426\n",
      "iteration 1000 / 5500: loss 1.982425\n",
      "iteration 1100 / 5500: loss 2.044515\n",
      "iteration 1200 / 5500: loss 2.043354\n",
      "iteration 1300 / 5500: loss 2.105650\n",
      "iteration 1400 / 5500: loss 1.987062\n",
      "iteration 1500 / 5500: loss 1.920875\n",
      "iteration 1600 / 5500: loss 2.020964\n",
      "iteration 1700 / 5500: loss 2.001510\n",
      "iteration 1800 / 5500: loss 2.031327\n",
      "iteration 1900 / 5500: loss 2.022806\n",
      "iteration 2000 / 5500: loss 2.051345\n",
      "iteration 2100 / 5500: loss 1.993819\n",
      "iteration 2200 / 5500: loss 2.024575\n",
      "iteration 2300 / 5500: loss 2.001350\n",
      "iteration 2400 / 5500: loss 1.968370\n",
      "iteration 2500 / 5500: loss 2.064022\n",
      "iteration 2600 / 5500: loss 2.038774\n",
      "iteration 2700 / 5500: loss 1.933086\n",
      "iteration 2800 / 5500: loss 2.082778\n",
      "iteration 2900 / 5500: loss 2.048734\n",
      "iteration 3000 / 5500: loss 2.005900\n",
      "iteration 3100 / 5500: loss 2.014568\n",
      "iteration 3200 / 5500: loss 1.974787\n",
      "iteration 3300 / 5500: loss 2.046207\n",
      "iteration 3400 / 5500: loss 2.014034\n",
      "iteration 3500 / 5500: loss 2.020682\n",
      "iteration 3600 / 5500: loss 1.963349\n",
      "iteration 3700 / 5500: loss 2.073121\n",
      "iteration 3800 / 5500: loss 1.999006\n",
      "iteration 3900 / 5500: loss 2.054061\n",
      "iteration 4000 / 5500: loss 2.032748\n",
      "iteration 4100 / 5500: loss 2.051139\n",
      "iteration 4200 / 5500: loss 2.024597\n",
      "iteration 4300 / 5500: loss 2.047602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4400 / 5500: loss 1.952167\n",
      "iteration 4500 / 5500: loss 2.005313\n",
      "iteration 4600 / 5500: loss 1.969110\n",
      "iteration 4700 / 5500: loss 1.978949\n",
      "iteration 4800 / 5500: loss 2.007038\n",
      "iteration 4900 / 5500: loss 1.923840\n",
      "iteration 5000 / 5500: loss 2.070780\n",
      "iteration 5100 / 5500: loss 1.947405\n",
      "iteration 5200 / 5500: loss 2.009103\n",
      "iteration 5300 / 5500: loss 2.111229\n",
      "iteration 5400 / 5500: loss 2.037369\n",
      "lr 5.000000e-07 reg 2.500000e+04\n",
      "iteration 0 / 5500: loss 779.196275\n",
      "iteration 100 / 5500: loss 6.916910\n",
      "iteration 200 / 5500: loss 2.034440\n",
      "iteration 300 / 5500: loss 2.064260\n",
      "iteration 400 / 5500: loss 2.077819\n",
      "iteration 500 / 5500: loss 2.129044\n",
      "iteration 600 / 5500: loss 2.091345\n",
      "iteration 700 / 5500: loss 2.106638\n",
      "iteration 800 / 5500: loss 2.133675\n",
      "iteration 900 / 5500: loss 2.093156\n",
      "iteration 1000 / 5500: loss 2.112155\n",
      "iteration 1100 / 5500: loss 2.017979\n",
      "iteration 1200 / 5500: loss 2.046052\n",
      "iteration 1300 / 5500: loss 2.117830\n",
      "iteration 1400 / 5500: loss 2.093842\n",
      "iteration 1500 / 5500: loss 2.066115\n",
      "iteration 1600 / 5500: loss 2.047646\n",
      "iteration 1700 / 5500: loss 2.080498\n",
      "iteration 1800 / 5500: loss 2.083020\n",
      "iteration 1900 / 5500: loss 2.118198\n",
      "iteration 2000 / 5500: loss 2.083167\n",
      "iteration 2100 / 5500: loss 2.082821\n",
      "iteration 2200 / 5500: loss 2.094370\n",
      "iteration 2300 / 5500: loss 2.106077\n",
      "iteration 2400 / 5500: loss 2.060828\n",
      "iteration 2500 / 5500: loss 2.128621\n",
      "iteration 2600 / 5500: loss 2.120654\n",
      "iteration 2700 / 5500: loss 2.091668\n",
      "iteration 2800 / 5500: loss 2.083680\n",
      "iteration 2900 / 5500: loss 2.077724\n",
      "iteration 3000 / 5500: loss 2.089681\n",
      "iteration 3100 / 5500: loss 2.047909\n",
      "iteration 3200 / 5500: loss 2.072255\n",
      "iteration 3300 / 5500: loss 2.043911\n",
      "iteration 3400 / 5500: loss 2.083093\n",
      "iteration 3500 / 5500: loss 2.136203\n",
      "iteration 3600 / 5500: loss 2.111160\n",
      "iteration 3700 / 5500: loss 2.070266\n",
      "iteration 3800 / 5500: loss 2.059367\n",
      "iteration 3900 / 5500: loss 2.066453\n",
      "iteration 4000 / 5500: loss 2.022971\n",
      "iteration 4100 / 5500: loss 2.073503\n",
      "iteration 4200 / 5500: loss 2.029176\n",
      "iteration 4300 / 5500: loss 2.099392\n",
      "iteration 4400 / 5500: loss 2.122030\n",
      "iteration 4500 / 5500: loss 2.096235\n",
      "iteration 4600 / 5500: loss 2.064267\n",
      "iteration 4700 / 5500: loss 2.051475\n",
      "iteration 4800 / 5500: loss 2.148941\n",
      "iteration 4900 / 5500: loss 2.083079\n",
      "iteration 5000 / 5500: loss 2.112164\n",
      "iteration 5100 / 5500: loss 2.092843\n",
      "iteration 5200 / 5500: loss 2.120009\n",
      "iteration 5300 / 5500: loss 2.075106\n",
      "iteration 5400 / 5500: loss 2.074750\n",
      "lr 5.000000e-07 reg 5.000000e+04\n",
      "iteration 0 / 5500: loss 1541.909924\n",
      "iteration 100 / 5500: loss 2.159918\n",
      "iteration 200 / 5500: loss 2.158751\n",
      "iteration 300 / 5500: loss 2.116337\n",
      "iteration 400 / 5500: loss 2.153790\n",
      "iteration 500 / 5500: loss 2.158290\n",
      "iteration 600 / 5500: loss 2.162768\n",
      "iteration 700 / 5500: loss 2.174389\n",
      "iteration 800 / 5500: loss 2.166848\n",
      "iteration 900 / 5500: loss 2.156759\n",
      "iteration 1000 / 5500: loss 2.170298\n",
      "iteration 1100 / 5500: loss 2.176209\n",
      "iteration 1200 / 5500: loss 2.158140\n",
      "iteration 1300 / 5500: loss 2.158143\n",
      "iteration 1400 / 5500: loss 2.180583\n",
      "iteration 1500 / 5500: loss 2.179040\n",
      "iteration 1600 / 5500: loss 2.130746\n",
      "iteration 1700 / 5500: loss 2.124216\n",
      "iteration 1800 / 5500: loss 2.097763\n",
      "iteration 1900 / 5500: loss 2.159229\n",
      "iteration 2000 / 5500: loss 2.144974\n",
      "iteration 2100 / 5500: loss 2.144653\n",
      "iteration 2200 / 5500: loss 2.145014\n",
      "iteration 2300 / 5500: loss 2.155325\n",
      "iteration 2400 / 5500: loss 2.156261\n",
      "iteration 2500 / 5500: loss 2.175413\n",
      "iteration 2600 / 5500: loss 2.149456\n",
      "iteration 2700 / 5500: loss 2.196357\n",
      "iteration 2800 / 5500: loss 2.110897\n",
      "iteration 2900 / 5500: loss 2.143397\n",
      "iteration 3000 / 5500: loss 2.174052\n",
      "iteration 3100 / 5500: loss 2.108101\n",
      "iteration 3200 / 5500: loss 2.103884\n",
      "iteration 3300 / 5500: loss 2.163284\n",
      "iteration 3400 / 5500: loss 2.141521\n",
      "iteration 3500 / 5500: loss 2.108034\n",
      "iteration 3600 / 5500: loss 2.137372\n",
      "iteration 3700 / 5500: loss 2.097233\n",
      "iteration 3800 / 5500: loss 2.137824\n",
      "iteration 3900 / 5500: loss 2.159499\n",
      "iteration 4000 / 5500: loss 2.176632\n",
      "iteration 4100 / 5500: loss 2.176210\n",
      "iteration 4200 / 5500: loss 2.159162\n",
      "iteration 4300 / 5500: loss 2.149985\n",
      "iteration 4400 / 5500: loss 2.133366\n",
      "iteration 4500 / 5500: loss 2.085591\n",
      "iteration 4600 / 5500: loss 2.165696\n",
      "iteration 4700 / 5500: loss 2.139251\n",
      "iteration 4800 / 5500: loss 2.156405\n",
      "iteration 4900 / 5500: loss 2.141144\n",
      "iteration 5000 / 5500: loss 2.125932\n",
      "iteration 5100 / 5500: loss 2.157342\n",
      "iteration 5200 / 5500: loss 2.131085\n",
      "iteration 5300 / 5500: loss 2.131171\n",
      "iteration 5400 / 5500: loss 2.154725\n",
      "lr 5.000000e-07 reg 1.000000e+05\n",
      "iteration 0 / 5500: loss 3106.795635\n",
      "iteration 100 / 5500: loss 2.199472\n",
      "iteration 200 / 5500: loss 2.191566\n",
      "iteration 300 / 5500: loss 2.184583\n",
      "iteration 400 / 5500: loss 2.195977\n",
      "iteration 500 / 5500: loss 2.201921\n",
      "iteration 600 / 5500: loss 2.201888\n",
      "iteration 700 / 5500: loss 2.202138\n",
      "iteration 800 / 5500: loss 2.200521\n",
      "iteration 900 / 5500: loss 2.204854\n",
      "iteration 1000 / 5500: loss 2.211069\n",
      "iteration 1100 / 5500: loss 2.217758\n",
      "iteration 1200 / 5500: loss 2.211300\n",
      "iteration 1300 / 5500: loss 2.225968\n",
      "iteration 1400 / 5500: loss 2.220152\n",
      "iteration 1500 / 5500: loss 2.207663\n",
      "iteration 1600 / 5500: loss 2.238848\n",
      "iteration 1700 / 5500: loss 2.232751\n",
      "iteration 1800 / 5500: loss 2.180811\n",
      "iteration 1900 / 5500: loss 2.174182\n",
      "iteration 2000 / 5500: loss 2.204617\n",
      "iteration 2100 / 5500: loss 2.190168\n",
      "iteration 2200 / 5500: loss 2.193357\n",
      "iteration 2300 / 5500: loss 2.194132\n",
      "iteration 2400 / 5500: loss 2.218295\n",
      "iteration 2500 / 5500: loss 2.184999\n",
      "iteration 2600 / 5500: loss 2.210456\n",
      "iteration 2700 / 5500: loss 2.213783\n",
      "iteration 2800 / 5500: loss 2.217375\n",
      "iteration 2900 / 5500: loss 2.167822\n",
      "iteration 3000 / 5500: loss 2.188356\n",
      "iteration 3100 / 5500: loss 2.202005\n",
      "iteration 3200 / 5500: loss 2.190316\n",
      "iteration 3300 / 5500: loss 2.204206\n",
      "iteration 3400 / 5500: loss 2.164486\n",
      "iteration 3500 / 5500: loss 2.205261\n",
      "iteration 3600 / 5500: loss 2.195842\n",
      "iteration 3700 / 5500: loss 2.184986\n",
      "iteration 3800 / 5500: loss 2.193248\n",
      "iteration 3900 / 5500: loss 2.194879\n",
      "iteration 4000 / 5500: loss 2.222893\n",
      "iteration 4100 / 5500: loss 2.166498\n",
      "iteration 4200 / 5500: loss 2.194807\n",
      "iteration 4300 / 5500: loss 2.201066\n",
      "iteration 4400 / 5500: loss 2.227679\n",
      "iteration 4500 / 5500: loss 2.185449\n",
      "iteration 4600 / 5500: loss 2.209322\n",
      "iteration 4700 / 5500: loss 2.217994\n",
      "iteration 4800 / 5500: loss 2.162784\n",
      "iteration 4900 / 5500: loss 2.199641\n",
      "iteration 5000 / 5500: loss 2.188415\n",
      "iteration 5100 / 5500: loss 2.207142\n",
      "iteration 5200 / 5500: loss 2.185096\n",
      "iteration 5300 / 5500: loss 2.176193\n",
      "iteration 5400 / 5500: loss 2.173603\n",
      "lr 5.000000e-07 reg 2.500000e+05\n",
      "iteration 0 / 5500: loss 7715.586160\n",
      "iteration 100 / 5500: loss 2.268452\n",
      "iteration 200 / 5500: loss 2.255909\n",
      "iteration 300 / 5500: loss 2.262285\n",
      "iteration 400 / 5500: loss 2.259499\n",
      "iteration 500 / 5500: loss 2.257321\n",
      "iteration 600 / 5500: loss 2.260432\n",
      "iteration 700 / 5500: loss 2.257894\n",
      "iteration 800 / 5500: loss 2.247313\n",
      "iteration 900 / 5500: loss 2.218408\n",
      "iteration 1000 / 5500: loss 2.251427\n",
      "iteration 1100 / 5500: loss 2.255583\n",
      "iteration 1200 / 5500: loss 2.230852\n",
      "iteration 1300 / 5500: loss 2.234507\n",
      "iteration 1400 / 5500: loss 2.229960\n",
      "iteration 1500 / 5500: loss 2.247643\n",
      "iteration 1600 / 5500: loss 2.272399\n",
      "iteration 1700 / 5500: loss 2.270017\n",
      "iteration 1800 / 5500: loss 2.256937\n",
      "iteration 1900 / 5500: loss 2.247740\n",
      "iteration 2000 / 5500: loss 2.260217\n",
      "iteration 2100 / 5500: loss 2.245135\n",
      "iteration 2200 / 5500: loss 2.239907\n",
      "iteration 2300 / 5500: loss 2.252402\n",
      "iteration 2400 / 5500: loss 2.257513\n",
      "iteration 2500 / 5500: loss 2.266606\n",
      "iteration 2600 / 5500: loss 2.246590\n",
      "iteration 2700 / 5500: loss 2.243309\n",
      "iteration 2800 / 5500: loss 2.249858\n",
      "iteration 2900 / 5500: loss 2.269272\n",
      "iteration 3000 / 5500: loss 2.235950\n",
      "iteration 3100 / 5500: loss 2.247483\n",
      "iteration 3200 / 5500: loss 2.241611\n",
      "iteration 3300 / 5500: loss 2.260497\n",
      "iteration 3400 / 5500: loss 2.238629\n",
      "iteration 3500 / 5500: loss 2.256297\n",
      "iteration 3600 / 5500: loss 2.249191\n",
      "iteration 3700 / 5500: loss 2.227784\n",
      "iteration 3800 / 5500: loss 2.280220\n",
      "iteration 3900 / 5500: loss 2.219840\n",
      "iteration 4000 / 5500: loss 2.251912\n",
      "iteration 4100 / 5500: loss 2.253854\n",
      "iteration 4200 / 5500: loss 2.249465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4300 / 5500: loss 2.246098\n",
      "iteration 4400 / 5500: loss 2.255299\n",
      "iteration 4500 / 5500: loss 2.257567\n",
      "iteration 4600 / 5500: loss 2.249124\n",
      "iteration 4700 / 5500: loss 2.232807\n",
      "iteration 4800 / 5500: loss 2.253751\n",
      "iteration 4900 / 5500: loss 2.245365\n",
      "iteration 5000 / 5500: loss 2.239384\n",
      "iteration 5100 / 5500: loss 2.271646\n",
      "iteration 5200 / 5500: loss 2.250397\n",
      "iteration 5300 / 5500: loss 2.250519\n",
      "iteration 5400 / 5500: loss 2.263854\n",
      "lr 1.000000e-06 reg 5.000000e+02\n",
      "iteration 0 / 5500: loss 20.454428\n",
      "iteration 100 / 5500: loss 15.034371\n",
      "iteration 200 / 5500: loss 12.467438\n",
      "iteration 300 / 5500: loss 10.236298\n",
      "iteration 400 / 5500: loss 8.581277\n",
      "iteration 500 / 5500: loss 7.454831\n",
      "iteration 600 / 5500: loss 6.209101\n",
      "iteration 700 / 5500: loss 5.407174\n",
      "iteration 800 / 5500: loss 4.748375\n",
      "iteration 900 / 5500: loss 4.223606\n",
      "iteration 1000 / 5500: loss 3.843300\n",
      "iteration 1100 / 5500: loss 3.397698\n",
      "iteration 1200 / 5500: loss 3.051915\n",
      "iteration 1300 / 5500: loss 2.751834\n",
      "iteration 1400 / 5500: loss 2.633857\n",
      "iteration 1500 / 5500: loss 2.529017\n",
      "iteration 1600 / 5500: loss 2.329715\n",
      "iteration 1700 / 5500: loss 2.147593\n",
      "iteration 1800 / 5500: loss 2.149556\n",
      "iteration 1900 / 5500: loss 2.031138\n",
      "iteration 2000 / 5500: loss 1.994603\n",
      "iteration 2100 / 5500: loss 2.114037\n",
      "iteration 2200 / 5500: loss 1.957482\n",
      "iteration 2300 / 5500: loss 1.900179\n",
      "iteration 2400 / 5500: loss 1.876202\n",
      "iteration 2500 / 5500: loss 1.831451\n",
      "iteration 2600 / 5500: loss 1.887414\n",
      "iteration 2700 / 5500: loss 1.809384\n",
      "iteration 2800 / 5500: loss 1.846252\n",
      "iteration 2900 / 5500: loss 1.796816\n",
      "iteration 3000 / 5500: loss 1.766944\n",
      "iteration 3100 / 5500: loss 1.800878\n",
      "iteration 3200 / 5500: loss 1.870995\n",
      "iteration 3300 / 5500: loss 1.720762\n",
      "iteration 3400 / 5500: loss 1.817125\n",
      "iteration 3500 / 5500: loss 1.773134\n",
      "iteration 3600 / 5500: loss 1.686199\n",
      "iteration 3700 / 5500: loss 1.727840\n",
      "iteration 3800 / 5500: loss 1.739192\n",
      "iteration 3900 / 5500: loss 1.797191\n",
      "iteration 4000 / 5500: loss 1.714877\n",
      "iteration 4100 / 5500: loss 1.763742\n",
      "iteration 4200 / 5500: loss 1.711385\n",
      "iteration 4300 / 5500: loss 1.780814\n",
      "iteration 4400 / 5500: loss 1.849275\n",
      "iteration 4500 / 5500: loss 1.797202\n",
      "iteration 4600 / 5500: loss 1.737036\n",
      "iteration 4700 / 5500: loss 1.751189\n",
      "iteration 4800 / 5500: loss 1.769531\n",
      "iteration 4900 / 5500: loss 1.713889\n",
      "iteration 5000 / 5500: loss 1.921014\n",
      "iteration 5100 / 5500: loss 1.827026\n",
      "iteration 5200 / 5500: loss 1.752788\n",
      "iteration 5300 / 5500: loss 1.778153\n",
      "iteration 5400 / 5500: loss 1.759518\n",
      "lr 1.000000e-06 reg 1.000000e+03\n",
      "iteration 0 / 5500: loss 36.293381\n",
      "iteration 100 / 5500: loss 23.122630\n",
      "iteration 200 / 5500: loss 15.878700\n",
      "iteration 300 / 5500: loss 11.027793\n",
      "iteration 400 / 5500: loss 7.883500\n",
      "iteration 500 / 5500: loss 5.948113\n",
      "iteration 600 / 5500: loss 4.702503\n",
      "iteration 700 / 5500: loss 3.575104\n",
      "iteration 800 / 5500: loss 3.023720\n",
      "iteration 900 / 5500: loss 2.680656\n",
      "iteration 1000 / 5500: loss 2.377765\n",
      "iteration 1100 / 5500: loss 2.136119\n",
      "iteration 1200 / 5500: loss 2.029699\n",
      "iteration 1300 / 5500: loss 1.970901\n",
      "iteration 1400 / 5500: loss 2.009465\n",
      "iteration 1500 / 5500: loss 1.914030\n",
      "iteration 1600 / 5500: loss 1.857097\n",
      "iteration 1700 / 5500: loss 2.029960\n",
      "iteration 1800 / 5500: loss 1.805538\n",
      "iteration 1900 / 5500: loss 1.808035\n",
      "iteration 2000 / 5500: loss 1.827206\n",
      "iteration 2100 / 5500: loss 1.775479\n",
      "iteration 2200 / 5500: loss 1.820303\n",
      "iteration 2300 / 5500: loss 1.889261\n",
      "iteration 2400 / 5500: loss 1.834643\n",
      "iteration 2500 / 5500: loss 1.757170\n",
      "iteration 2600 / 5500: loss 1.815133\n",
      "iteration 2700 / 5500: loss 1.731617\n",
      "iteration 2800 / 5500: loss 1.880542\n",
      "iteration 2900 / 5500: loss 1.878713\n",
      "iteration 3000 / 5500: loss 1.896553\n",
      "iteration 3100 / 5500: loss 1.815154\n",
      "iteration 3200 / 5500: loss 1.866115\n",
      "iteration 3300 / 5500: loss 1.759556\n",
      "iteration 3400 / 5500: loss 1.777687\n",
      "iteration 3500 / 5500: loss 1.908383\n",
      "iteration 3600 / 5500: loss 1.815426\n",
      "iteration 3700 / 5500: loss 1.901039\n",
      "iteration 3800 / 5500: loss 1.812815\n",
      "iteration 3900 / 5500: loss 1.848941\n",
      "iteration 4000 / 5500: loss 1.856549\n",
      "iteration 4100 / 5500: loss 1.929090\n",
      "iteration 4200 / 5500: loss 1.870432\n",
      "iteration 4300 / 5500: loss 1.789688\n",
      "iteration 4400 / 5500: loss 1.831533\n",
      "iteration 4500 / 5500: loss 1.813048\n",
      "iteration 4600 / 5500: loss 1.890119\n",
      "iteration 4700 / 5500: loss 1.824629\n",
      "iteration 4800 / 5500: loss 1.905261\n",
      "iteration 4900 / 5500: loss 1.864288\n",
      "iteration 5000 / 5500: loss 1.795109\n",
      "iteration 5100 / 5500: loss 1.788236\n",
      "iteration 5200 / 5500: loss 1.888839\n",
      "iteration 5300 / 5500: loss 1.738034\n",
      "iteration 5400 / 5500: loss 1.862713\n",
      "lr 1.000000e-06 reg 5.000000e+03\n",
      "iteration 0 / 5500: loss 162.404836\n",
      "iteration 100 / 5500: loss 22.550070\n",
      "iteration 200 / 5500: loss 4.648152\n",
      "iteration 300 / 5500: loss 2.219568\n",
      "iteration 400 / 5500: loss 2.049363\n",
      "iteration 500 / 5500: loss 1.979544\n",
      "iteration 600 / 5500: loss 1.846685\n",
      "iteration 700 / 5500: loss 1.948513\n",
      "iteration 800 / 5500: loss 1.944389\n",
      "iteration 900 / 5500: loss 2.041277\n",
      "iteration 1000 / 5500: loss 2.054571\n",
      "iteration 1100 / 5500: loss 1.996305\n",
      "iteration 1200 / 5500: loss 1.862044\n",
      "iteration 1300 / 5500: loss 2.020151\n",
      "iteration 1400 / 5500: loss 1.963775\n",
      "iteration 1500 / 5500: loss 1.948024\n",
      "iteration 1600 / 5500: loss 1.881970\n",
      "iteration 1700 / 5500: loss 1.917487\n",
      "iteration 1800 / 5500: loss 1.936633\n",
      "iteration 1900 / 5500: loss 2.012217\n",
      "iteration 2000 / 5500: loss 1.996905\n",
      "iteration 2100 / 5500: loss 1.942084\n",
      "iteration 2200 / 5500: loss 1.943402\n",
      "iteration 2300 / 5500: loss 1.985222\n",
      "iteration 2400 / 5500: loss 1.911940\n",
      "iteration 2500 / 5500: loss 1.997017\n",
      "iteration 2600 / 5500: loss 2.047039\n",
      "iteration 2700 / 5500: loss 1.853002\n",
      "iteration 2800 / 5500: loss 1.899127\n",
      "iteration 2900 / 5500: loss 1.940835\n",
      "iteration 3000 / 5500: loss 1.923553\n",
      "iteration 3100 / 5500: loss 1.908162\n",
      "iteration 3200 / 5500: loss 2.042741\n",
      "iteration 3300 / 5500: loss 1.926444\n",
      "iteration 3400 / 5500: loss 2.012489\n",
      "iteration 3500 / 5500: loss 1.999623\n",
      "iteration 3600 / 5500: loss 1.956638\n",
      "iteration 3700 / 5500: loss 1.905527\n",
      "iteration 3800 / 5500: loss 2.004217\n",
      "iteration 3900 / 5500: loss 1.922933\n",
      "iteration 4000 / 5500: loss 1.905431\n",
      "iteration 4100 / 5500: loss 1.963045\n",
      "iteration 4200 / 5500: loss 1.955339\n",
      "iteration 4300 / 5500: loss 1.835382\n",
      "iteration 4400 / 5500: loss 1.948311\n",
      "iteration 4500 / 5500: loss 2.054537\n",
      "iteration 4600 / 5500: loss 1.862602\n",
      "iteration 4700 / 5500: loss 1.971940\n",
      "iteration 4800 / 5500: loss 2.016296\n",
      "iteration 4900 / 5500: loss 2.032078\n",
      "iteration 5000 / 5500: loss 1.997252\n",
      "iteration 5100 / 5500: loss 1.917356\n",
      "iteration 5200 / 5500: loss 2.031842\n",
      "iteration 5300 / 5500: loss 1.933492\n",
      "iteration 5400 / 5500: loss 1.891676\n",
      "lr 1.000000e-06 reg 1.000000e+04\n",
      "iteration 0 / 5500: loss 313.933514\n",
      "iteration 100 / 5500: loss 7.330861\n",
      "iteration 200 / 5500: loss 2.092056\n",
      "iteration 300 / 5500: loss 2.079113\n",
      "iteration 400 / 5500: loss 1.989588\n",
      "iteration 500 / 5500: loss 1.977917\n",
      "iteration 600 / 5500: loss 2.081912\n",
      "iteration 700 / 5500: loss 2.058734\n",
      "iteration 800 / 5500: loss 2.045819\n",
      "iteration 900 / 5500: loss 2.032227\n",
      "iteration 1000 / 5500: loss 1.988487\n",
      "iteration 1100 / 5500: loss 2.101130\n",
      "iteration 1200 / 5500: loss 2.033797\n",
      "iteration 1300 / 5500: loss 1.988091\n",
      "iteration 1400 / 5500: loss 2.045974\n",
      "iteration 1500 / 5500: loss 2.064508\n",
      "iteration 1600 / 5500: loss 1.982347\n",
      "iteration 1700 / 5500: loss 2.042486\n",
      "iteration 1800 / 5500: loss 2.045621\n",
      "iteration 1900 / 5500: loss 2.024000\n",
      "iteration 2000 / 5500: loss 2.075992\n",
      "iteration 2100 / 5500: loss 2.047172\n",
      "iteration 2200 / 5500: loss 2.073800\n",
      "iteration 2300 / 5500: loss 2.062956\n",
      "iteration 2400 / 5500: loss 2.015732\n",
      "iteration 2500 / 5500: loss 2.035859\n",
      "iteration 2600 / 5500: loss 1.965741\n",
      "iteration 2700 / 5500: loss 1.962687\n",
      "iteration 2800 / 5500: loss 2.050795\n",
      "iteration 2900 / 5500: loss 2.018124\n",
      "iteration 3000 / 5500: loss 1.987504\n",
      "iteration 3100 / 5500: loss 2.011953\n",
      "iteration 3200 / 5500: loss 2.007935\n",
      "iteration 3300 / 5500: loss 1.881337\n",
      "iteration 3400 / 5500: loss 2.139229\n",
      "iteration 3500 / 5500: loss 2.008697\n",
      "iteration 3600 / 5500: loss 2.022972\n",
      "iteration 3700 / 5500: loss 2.085917\n",
      "iteration 3800 / 5500: loss 2.014391\n",
      "iteration 3900 / 5500: loss 1.962556\n",
      "iteration 4000 / 5500: loss 1.938919\n",
      "iteration 4100 / 5500: loss 1.986191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4200 / 5500: loss 2.092547\n",
      "iteration 4300 / 5500: loss 2.117050\n",
      "iteration 4400 / 5500: loss 2.063844\n",
      "iteration 4500 / 5500: loss 1.973818\n",
      "iteration 4600 / 5500: loss 2.051567\n",
      "iteration 4700 / 5500: loss 2.010606\n",
      "iteration 4800 / 5500: loss 1.999421\n",
      "iteration 4900 / 5500: loss 2.082164\n",
      "iteration 5000 / 5500: loss 2.000839\n",
      "iteration 5100 / 5500: loss 1.994336\n",
      "iteration 5200 / 5500: loss 2.018139\n",
      "iteration 5300 / 5500: loss 2.041213\n",
      "iteration 5400 / 5500: loss 1.990109\n",
      "lr 1.000000e-06 reg 2.500000e+04\n",
      "iteration 0 / 5500: loss 791.086998\n",
      "iteration 100 / 5500: loss 2.113140\n",
      "iteration 200 / 5500: loss 2.135206\n",
      "iteration 300 / 5500: loss 2.053621\n",
      "iteration 400 / 5500: loss 2.109148\n",
      "iteration 500 / 5500: loss 2.119821\n",
      "iteration 600 / 5500: loss 2.074005\n",
      "iteration 700 / 5500: loss 2.156502\n",
      "iteration 800 / 5500: loss 2.048748\n",
      "iteration 900 / 5500: loss 2.092030\n",
      "iteration 1000 / 5500: loss 2.085502\n",
      "iteration 1100 / 5500: loss 2.093870\n",
      "iteration 1200 / 5500: loss 2.106695\n",
      "iteration 1300 / 5500: loss 2.104751\n",
      "iteration 1400 / 5500: loss 2.080207\n",
      "iteration 1500 / 5500: loss 2.033787\n",
      "iteration 1600 / 5500: loss 2.051540\n",
      "iteration 1700 / 5500: loss 2.093466\n",
      "iteration 1800 / 5500: loss 2.133543\n",
      "iteration 1900 / 5500: loss 2.113001\n",
      "iteration 2000 / 5500: loss 2.056053\n",
      "iteration 2100 / 5500: loss 2.125733\n",
      "iteration 2200 / 5500: loss 2.082691\n",
      "iteration 2300 / 5500: loss 2.107915\n",
      "iteration 2400 / 5500: loss 2.131448\n",
      "iteration 2500 / 5500: loss 2.117417\n",
      "iteration 2600 / 5500: loss 2.056811\n",
      "iteration 2700 / 5500: loss 2.086572\n",
      "iteration 2800 / 5500: loss 2.099819\n",
      "iteration 2900 / 5500: loss 2.099991\n",
      "iteration 3000 / 5500: loss 2.045864\n",
      "iteration 3100 / 5500: loss 2.046819\n",
      "iteration 3200 / 5500: loss 2.066475\n",
      "iteration 3300 / 5500: loss 2.059516\n",
      "iteration 3400 / 5500: loss 2.075536\n",
      "iteration 3500 / 5500: loss 2.106765\n",
      "iteration 3600 / 5500: loss 2.103016\n",
      "iteration 3700 / 5500: loss 2.095436\n",
      "iteration 3800 / 5500: loss 2.121653\n",
      "iteration 3900 / 5500: loss 2.181023\n",
      "iteration 4000 / 5500: loss 2.122320\n",
      "iteration 4100 / 5500: loss 2.044086\n",
      "iteration 4200 / 5500: loss 2.081613\n",
      "iteration 4300 / 5500: loss 2.086639\n",
      "iteration 4400 / 5500: loss 2.063628\n",
      "iteration 4500 / 5500: loss 2.122828\n",
      "iteration 4600 / 5500: loss 2.087678\n",
      "iteration 4700 / 5500: loss 2.088022\n",
      "iteration 4800 / 5500: loss 2.187302\n",
      "iteration 4900 / 5500: loss 2.026561\n",
      "iteration 5000 / 5500: loss 2.072500\n",
      "iteration 5100 / 5500: loss 2.121315\n",
      "iteration 5200 / 5500: loss 2.076034\n",
      "iteration 5300 / 5500: loss 2.131839\n",
      "iteration 5400 / 5500: loss 2.130795\n",
      "lr 1.000000e-06 reg 5.000000e+04\n",
      "iteration 0 / 5500: loss 1548.550733\n",
      "iteration 100 / 5500: loss 2.220085\n",
      "iteration 200 / 5500: loss 2.132425\n",
      "iteration 300 / 5500: loss 2.186274\n",
      "iteration 400 / 5500: loss 2.153882\n",
      "iteration 500 / 5500: loss 2.105236\n",
      "iteration 600 / 5500: loss 2.213191\n",
      "iteration 700 / 5500: loss 2.135891\n",
      "iteration 800 / 5500: loss 2.221710\n",
      "iteration 900 / 5500: loss 2.159230\n",
      "iteration 1000 / 5500: loss 2.142082\n",
      "iteration 1100 / 5500: loss 2.175586\n",
      "iteration 1200 / 5500: loss 2.134241\n",
      "iteration 1300 / 5500: loss 2.159265\n",
      "iteration 1400 / 5500: loss 2.150032\n",
      "iteration 1500 / 5500: loss 2.152291\n",
      "iteration 1600 / 5500: loss 2.135840\n",
      "iteration 1700 / 5500: loss 2.138042\n",
      "iteration 1800 / 5500: loss 2.197782\n",
      "iteration 1900 / 5500: loss 2.151242\n",
      "iteration 2000 / 5500: loss 2.143853\n",
      "iteration 2100 / 5500: loss 2.164863\n",
      "iteration 2200 / 5500: loss 2.158944\n",
      "iteration 2300 / 5500: loss 2.135210\n",
      "iteration 2400 / 5500: loss 2.217865\n",
      "iteration 2500 / 5500: loss 2.126783\n",
      "iteration 2600 / 5500: loss 2.161966\n",
      "iteration 2700 / 5500: loss 2.125605\n",
      "iteration 2800 / 5500: loss 2.112454\n",
      "iteration 2900 / 5500: loss 2.169251\n",
      "iteration 3000 / 5500: loss 2.155304\n",
      "iteration 3100 / 5500: loss 2.183172\n",
      "iteration 3200 / 5500: loss 2.159567\n",
      "iteration 3300 / 5500: loss 2.172387\n",
      "iteration 3400 / 5500: loss 2.158525\n",
      "iteration 3500 / 5500: loss 2.133401\n",
      "iteration 3600 / 5500: loss 2.184848\n",
      "iteration 3700 / 5500: loss 2.145367\n",
      "iteration 3800 / 5500: loss 2.134442\n",
      "iteration 3900 / 5500: loss 2.147136\n",
      "iteration 4000 / 5500: loss 2.176512\n",
      "iteration 4100 / 5500: loss 2.131942\n",
      "iteration 4200 / 5500: loss 2.197872\n",
      "iteration 4300 / 5500: loss 2.148334\n",
      "iteration 4400 / 5500: loss 2.173276\n",
      "iteration 4500 / 5500: loss 2.100352\n",
      "iteration 4600 / 5500: loss 2.141901\n",
      "iteration 4700 / 5500: loss 2.164236\n",
      "iteration 4800 / 5500: loss 2.140596\n",
      "iteration 4900 / 5500: loss 2.132262\n",
      "iteration 5000 / 5500: loss 2.163853\n",
      "iteration 5100 / 5500: loss 2.199357\n",
      "iteration 5200 / 5500: loss 2.206789\n",
      "iteration 5300 / 5500: loss 2.158431\n",
      "iteration 5400 / 5500: loss 2.166759\n",
      "lr 1.000000e-06 reg 1.000000e+05\n",
      "iteration 0 / 5500: loss 3063.050913\n",
      "iteration 100 / 5500: loss 2.240661\n",
      "iteration 200 / 5500: loss 2.205816\n",
      "iteration 300 / 5500: loss 2.208940\n",
      "iteration 400 / 5500: loss 2.181381\n",
      "iteration 500 / 5500: loss 2.202806\n",
      "iteration 600 / 5500: loss 2.223699\n",
      "iteration 700 / 5500: loss 2.225204\n",
      "iteration 800 / 5500: loss 2.205349\n",
      "iteration 900 / 5500: loss 2.208250\n",
      "iteration 1000 / 5500: loss 2.204915\n",
      "iteration 1100 / 5500: loss 2.178925\n",
      "iteration 1200 / 5500: loss 2.197963\n",
      "iteration 1300 / 5500: loss 2.251908\n",
      "iteration 1400 / 5500: loss 2.245233\n",
      "iteration 1500 / 5500: loss 2.208467\n",
      "iteration 1600 / 5500: loss 2.230383\n",
      "iteration 1700 / 5500: loss 2.193969\n",
      "iteration 1800 / 5500: loss 2.195918\n",
      "iteration 1900 / 5500: loss 2.223959\n",
      "iteration 2000 / 5500: loss 2.245995\n",
      "iteration 2100 / 5500: loss 2.163550\n",
      "iteration 2200 / 5500: loss 2.216856\n",
      "iteration 2300 / 5500: loss 2.219043\n",
      "iteration 2400 / 5500: loss 2.231757\n",
      "iteration 2500 / 5500: loss 2.193232\n",
      "iteration 2600 / 5500: loss 2.174300\n",
      "iteration 2700 / 5500: loss 2.220356\n",
      "iteration 2800 / 5500: loss 2.218735\n",
      "iteration 2900 / 5500: loss 2.238240\n",
      "iteration 3000 / 5500: loss 2.215524\n",
      "iteration 3100 / 5500: loss 2.206851\n",
      "iteration 3200 / 5500: loss 2.210455\n",
      "iteration 3300 / 5500: loss 2.190580\n",
      "iteration 3400 / 5500: loss 2.197096\n",
      "iteration 3500 / 5500: loss 2.229053\n",
      "iteration 3600 / 5500: loss 2.179776\n",
      "iteration 3700 / 5500: loss 2.251215\n",
      "iteration 3800 / 5500: loss 2.195852\n",
      "iteration 3900 / 5500: loss 2.182963\n",
      "iteration 4000 / 5500: loss 2.195177\n",
      "iteration 4100 / 5500: loss 2.177993\n",
      "iteration 4200 / 5500: loss 2.237781\n",
      "iteration 4300 / 5500: loss 2.157702\n",
      "iteration 4400 / 5500: loss 2.220953\n",
      "iteration 4500 / 5500: loss 2.200513\n",
      "iteration 4600 / 5500: loss 2.210322\n",
      "iteration 4700 / 5500: loss 2.197239\n",
      "iteration 4800 / 5500: loss 2.203134\n",
      "iteration 4900 / 5500: loss 2.175765\n",
      "iteration 5000 / 5500: loss 2.240838\n",
      "iteration 5100 / 5500: loss 2.219251\n",
      "iteration 5200 / 5500: loss 2.193287\n",
      "iteration 5300 / 5500: loss 2.207498\n",
      "iteration 5400 / 5500: loss 2.180867\n",
      "lr 1.000000e-06 reg 2.500000e+05\n",
      "iteration 0 / 5500: loss 7773.379565\n",
      "iteration 100 / 5500: loss 2.255266\n",
      "iteration 200 / 5500: loss 2.281810\n",
      "iteration 300 / 5500: loss 2.261776\n",
      "iteration 400 / 5500: loss 2.263486\n",
      "iteration 500 / 5500: loss 2.250391\n",
      "iteration 600 / 5500: loss 2.268181\n",
      "iteration 700 / 5500: loss 2.258004\n",
      "iteration 800 / 5500: loss 2.271851\n",
      "iteration 900 / 5500: loss 2.277561\n",
      "iteration 1000 / 5500: loss 2.240858\n",
      "iteration 1100 / 5500: loss 2.261254\n",
      "iteration 1200 / 5500: loss 2.280817\n",
      "iteration 1300 / 5500: loss 2.248100\n",
      "iteration 1400 / 5500: loss 2.274989\n",
      "iteration 1500 / 5500: loss 2.279537\n",
      "iteration 1600 / 5500: loss 2.255501\n",
      "iteration 1700 / 5500: loss 2.265483\n",
      "iteration 1800 / 5500: loss 2.249311\n",
      "iteration 1900 / 5500: loss 2.248342\n",
      "iteration 2000 / 5500: loss 2.265369\n",
      "iteration 2100 / 5500: loss 2.255076\n",
      "iteration 2200 / 5500: loss 2.276305\n",
      "iteration 2300 / 5500: loss 2.275292\n",
      "iteration 2400 / 5500: loss 2.284022\n",
      "iteration 2500 / 5500: loss 2.262364\n",
      "iteration 2600 / 5500: loss 2.269844\n",
      "iteration 2700 / 5500: loss 2.278552\n",
      "iteration 2800 / 5500: loss 2.273108\n",
      "iteration 2900 / 5500: loss 2.287519\n",
      "iteration 3000 / 5500: loss 2.273620\n",
      "iteration 3100 / 5500: loss 2.273117\n",
      "iteration 3200 / 5500: loss 2.258109\n",
      "iteration 3300 / 5500: loss 2.249443\n",
      "iteration 3400 / 5500: loss 2.241528\n",
      "iteration 3500 / 5500: loss 2.261607\n",
      "iteration 3600 / 5500: loss 2.261622\n",
      "iteration 3700 / 5500: loss 2.258858\n",
      "iteration 3800 / 5500: loss 2.263242\n",
      "iteration 3900 / 5500: loss 2.254445\n",
      "iteration 4000 / 5500: loss 2.264195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4100 / 5500: loss 2.259663\n",
      "iteration 4200 / 5500: loss 2.249006\n",
      "iteration 4300 / 5500: loss 2.245388\n",
      "iteration 4400 / 5500: loss 2.269993\n",
      "iteration 4500 / 5500: loss 2.253620\n",
      "iteration 4600 / 5500: loss 2.280331\n",
      "iteration 4700 / 5500: loss 2.279011\n",
      "iteration 4800 / 5500: loss 2.268921\n",
      "iteration 4900 / 5500: loss 2.259666\n",
      "iteration 5000 / 5500: loss 2.279037\n",
      "iteration 5100 / 5500: loss 2.259308\n",
      "iteration 5200 / 5500: loss 2.268356\n",
      "iteration 5300 / 5500: loss 2.253805\n",
      "iteration 5400 / 5500: loss 2.266330\n",
      "lr 5.000000e-06 reg 5.000000e+02\n",
      "iteration 0 / 5500: loss 20.258183\n",
      "iteration 100 / 5500: loss 7.530748\n",
      "iteration 200 / 5500: loss 3.813126\n",
      "iteration 300 / 5500: loss 2.570840\n",
      "iteration 400 / 5500: loss 2.116298\n",
      "iteration 500 / 5500: loss 2.030181\n",
      "iteration 600 / 5500: loss 1.952976\n",
      "iteration 700 / 5500: loss 2.018075\n",
      "iteration 800 / 5500: loss 2.023351\n",
      "iteration 900 / 5500: loss 1.962775\n",
      "iteration 1000 / 5500: loss 1.868773\n",
      "iteration 1100 / 5500: loss 1.807756\n",
      "iteration 1200 / 5500: loss 1.849055\n",
      "iteration 1300 / 5500: loss 1.929366\n",
      "iteration 1400 / 5500: loss 1.940550\n",
      "iteration 1500 / 5500: loss 1.708846\n",
      "iteration 1600 / 5500: loss 1.895565\n",
      "iteration 1700 / 5500: loss 1.833759\n",
      "iteration 1800 / 5500: loss 1.926756\n",
      "iteration 1900 / 5500: loss 1.973704\n",
      "iteration 2000 / 5500: loss 1.864788\n",
      "iteration 2100 / 5500: loss 2.096384\n",
      "iteration 2200 / 5500: loss 2.039628\n",
      "iteration 2300 / 5500: loss 1.964756\n",
      "iteration 2400 / 5500: loss 1.935999\n",
      "iteration 2500 / 5500: loss 1.924520\n",
      "iteration 2600 / 5500: loss 1.885862\n",
      "iteration 2700 / 5500: loss 1.830520\n",
      "iteration 2800 / 5500: loss 2.003692\n",
      "iteration 2900 / 5500: loss 2.169204\n",
      "iteration 3000 / 5500: loss 2.011053\n",
      "iteration 3100 / 5500: loss 1.936682\n",
      "iteration 3200 / 5500: loss 2.085504\n",
      "iteration 3300 / 5500: loss 1.899469\n",
      "iteration 3400 / 5500: loss 1.859157\n",
      "iteration 3500 / 5500: loss 2.002868\n",
      "iteration 3600 / 5500: loss 1.755837\n",
      "iteration 3700 / 5500: loss 1.769736\n",
      "iteration 3800 / 5500: loss 1.841548\n",
      "iteration 3900 / 5500: loss 1.888097\n",
      "iteration 4000 / 5500: loss 1.828820\n",
      "iteration 4100 / 5500: loss 1.808153\n",
      "iteration 4200 / 5500: loss 1.878221\n",
      "iteration 4300 / 5500: loss 1.914699\n",
      "iteration 4400 / 5500: loss 1.944803\n",
      "iteration 4500 / 5500: loss 1.757979\n",
      "iteration 4600 / 5500: loss 1.967295\n",
      "iteration 4700 / 5500: loss 1.976183\n",
      "iteration 4800 / 5500: loss 1.958909\n",
      "iteration 4900 / 5500: loss 2.075811\n",
      "iteration 5000 / 5500: loss 1.911221\n",
      "iteration 5100 / 5500: loss 1.919042\n",
      "iteration 5200 / 5500: loss 2.082571\n",
      "iteration 5300 / 5500: loss 2.056010\n",
      "iteration 5400 / 5500: loss 2.039164\n",
      "lr 5.000000e-06 reg 1.000000e+03\n",
      "iteration 0 / 5500: loss 36.376399\n",
      "iteration 100 / 5500: loss 6.160124\n",
      "iteration 200 / 5500: loss 2.349368\n",
      "iteration 300 / 5500: loss 2.053225\n",
      "iteration 400 / 5500: loss 1.924093\n",
      "iteration 500 / 5500: loss 1.884367\n",
      "iteration 600 / 5500: loss 1.954829\n",
      "iteration 700 / 5500: loss 2.236413\n",
      "iteration 800 / 5500: loss 1.861572\n",
      "iteration 900 / 5500: loss 2.147909\n",
      "iteration 1000 / 5500: loss 2.103779\n",
      "iteration 1100 / 5500: loss 2.028605\n",
      "iteration 1200 / 5500: loss 2.013873\n",
      "iteration 1300 / 5500: loss 1.921404\n",
      "iteration 1400 / 5500: loss 1.770276\n",
      "iteration 1500 / 5500: loss 2.073985\n",
      "iteration 1600 / 5500: loss 1.897457\n",
      "iteration 1700 / 5500: loss 2.095212\n",
      "iteration 1800 / 5500: loss 2.099447\n",
      "iteration 1900 / 5500: loss 1.944593\n",
      "iteration 2000 / 5500: loss 1.986329\n",
      "iteration 2100 / 5500: loss 1.940461\n",
      "iteration 2200 / 5500: loss 1.911981\n",
      "iteration 2300 / 5500: loss 1.921520\n",
      "iteration 2400 / 5500: loss 2.311981\n",
      "iteration 2500 / 5500: loss 1.882881\n",
      "iteration 2600 / 5500: loss 2.106900\n",
      "iteration 2700 / 5500: loss 2.088200\n",
      "iteration 2800 / 5500: loss 1.974495\n",
      "iteration 2900 / 5500: loss 1.868130\n",
      "iteration 3000 / 5500: loss 1.842508\n",
      "iteration 3100 / 5500: loss 1.920733\n",
      "iteration 3200 / 5500: loss 2.119355\n",
      "iteration 3300 / 5500: loss 1.867344\n",
      "iteration 3400 / 5500: loss 1.813502\n",
      "iteration 3500 / 5500: loss 2.042338\n",
      "iteration 3600 / 5500: loss 1.843077\n",
      "iteration 3700 / 5500: loss 1.895894\n",
      "iteration 3800 / 5500: loss 1.958741\n",
      "iteration 3900 / 5500: loss 1.838342\n",
      "iteration 4000 / 5500: loss 2.045278\n",
      "iteration 4100 / 5500: loss 1.894348\n",
      "iteration 4200 / 5500: loss 1.902121\n",
      "iteration 4300 / 5500: loss 2.045813\n",
      "iteration 4400 / 5500: loss 1.887233\n",
      "iteration 4500 / 5500: loss 2.003264\n",
      "iteration 4600 / 5500: loss 2.131493\n",
      "iteration 4700 / 5500: loss 1.989199\n",
      "iteration 4800 / 5500: loss 2.290592\n",
      "iteration 4900 / 5500: loss 2.057161\n",
      "iteration 5000 / 5500: loss 2.163209\n",
      "iteration 5100 / 5500: loss 1.928259\n",
      "iteration 5200 / 5500: loss 1.932362\n",
      "iteration 5300 / 5500: loss 2.142877\n",
      "iteration 5400 / 5500: loss 1.871750\n",
      "lr 5.000000e-06 reg 5.000000e+03\n",
      "iteration 0 / 5500: loss 161.033993\n",
      "iteration 100 / 5500: loss 2.362837\n",
      "iteration 200 / 5500: loss 2.142605\n",
      "iteration 300 / 5500: loss 2.149781\n",
      "iteration 400 / 5500: loss 2.060286\n",
      "iteration 500 / 5500: loss 2.162667\n",
      "iteration 600 / 5500: loss 2.187079\n",
      "iteration 700 / 5500: loss 2.163704\n",
      "iteration 800 / 5500: loss 2.201949\n",
      "iteration 900 / 5500: loss 2.124263\n",
      "iteration 1000 / 5500: loss 2.040635\n",
      "iteration 1100 / 5500: loss 2.065856\n",
      "iteration 1200 / 5500: loss 2.177480\n",
      "iteration 1300 / 5500: loss 2.031288\n",
      "iteration 1400 / 5500: loss 1.975900\n",
      "iteration 1500 / 5500: loss 2.440725\n",
      "iteration 1600 / 5500: loss 2.068673\n",
      "iteration 1700 / 5500: loss 2.017078\n",
      "iteration 1800 / 5500: loss 2.182957\n",
      "iteration 1900 / 5500: loss 1.996940\n",
      "iteration 2000 / 5500: loss 1.929084\n",
      "iteration 2100 / 5500: loss 2.122724\n",
      "iteration 2200 / 5500: loss 2.105230\n",
      "iteration 2300 / 5500: loss 2.085442\n",
      "iteration 2400 / 5500: loss 2.363221\n",
      "iteration 2500 / 5500: loss 2.293412\n",
      "iteration 2600 / 5500: loss 2.168287\n",
      "iteration 2700 / 5500: loss 2.167038\n",
      "iteration 2800 / 5500: loss 2.055643\n",
      "iteration 2900 / 5500: loss 2.297507\n",
      "iteration 3000 / 5500: loss 2.071443\n",
      "iteration 3100 / 5500: loss 2.186457\n",
      "iteration 3200 / 5500: loss 2.106766\n",
      "iteration 3300 / 5500: loss 2.023522\n",
      "iteration 3400 / 5500: loss 2.157193\n",
      "iteration 3500 / 5500: loss 2.079902\n",
      "iteration 3600 / 5500: loss 2.328747\n",
      "iteration 3700 / 5500: loss 2.072886\n",
      "iteration 3800 / 5500: loss 2.322237\n",
      "iteration 3900 / 5500: loss 2.040528\n",
      "iteration 4000 / 5500: loss 2.220898\n",
      "iteration 4100 / 5500: loss 2.124282\n",
      "iteration 4200 / 5500: loss 2.200083\n",
      "iteration 4300 / 5500: loss 2.030169\n",
      "iteration 4400 / 5500: loss 2.067823\n",
      "iteration 4500 / 5500: loss 2.011729\n",
      "iteration 4600 / 5500: loss 2.043323\n",
      "iteration 4700 / 5500: loss 2.026541\n",
      "iteration 4800 / 5500: loss 2.028223\n",
      "iteration 4900 / 5500: loss 2.139476\n",
      "iteration 5000 / 5500: loss 2.148821\n",
      "iteration 5100 / 5500: loss 2.223999\n",
      "iteration 5200 / 5500: loss 2.037070\n",
      "iteration 5300 / 5500: loss 2.109211\n",
      "iteration 5400 / 5500: loss 2.348875\n",
      "lr 5.000000e-06 reg 1.000000e+04\n",
      "iteration 0 / 5500: loss 315.555339\n",
      "iteration 100 / 5500: loss 2.244310\n",
      "iteration 200 / 5500: loss 2.440998\n",
      "iteration 300 / 5500: loss 2.907486\n",
      "iteration 400 / 5500: loss 2.201131\n",
      "iteration 500 / 5500: loss 2.212046\n",
      "iteration 600 / 5500: loss 2.302484\n",
      "iteration 700 / 5500: loss 2.145951\n",
      "iteration 800 / 5500: loss 2.199529\n",
      "iteration 900 / 5500: loss 2.352120\n",
      "iteration 1000 / 5500: loss 2.299083\n",
      "iteration 1100 / 5500: loss 2.034415\n",
      "iteration 1200 / 5500: loss 2.072772\n",
      "iteration 1300 / 5500: loss 2.100163\n",
      "iteration 1400 / 5500: loss 2.104571\n",
      "iteration 1500 / 5500: loss 2.183188\n",
      "iteration 1600 / 5500: loss 2.208864\n",
      "iteration 1700 / 5500: loss 2.110476\n",
      "iteration 1800 / 5500: loss 2.290989\n",
      "iteration 1900 / 5500: loss 2.139179\n",
      "iteration 2000 / 5500: loss 2.229055\n",
      "iteration 2100 / 5500: loss 2.511284\n",
      "iteration 2200 / 5500: loss 2.121872\n",
      "iteration 2300 / 5500: loss 2.409296\n",
      "iteration 2400 / 5500: loss 2.173380\n",
      "iteration 2500 / 5500: loss 2.198369\n",
      "iteration 2600 / 5500: loss 2.105369\n",
      "iteration 2700 / 5500: loss 2.057508\n",
      "iteration 2800 / 5500: loss 2.178727\n",
      "iteration 2900 / 5500: loss 2.204466\n",
      "iteration 3000 / 5500: loss 2.306248\n",
      "iteration 3100 / 5500: loss 2.175450\n",
      "iteration 3200 / 5500: loss 2.375827\n",
      "iteration 3300 / 5500: loss 2.220001\n",
      "iteration 3400 / 5500: loss 2.275800\n",
      "iteration 3500 / 5500: loss 2.217033\n",
      "iteration 3600 / 5500: loss 2.236986\n",
      "iteration 3700 / 5500: loss 2.315261\n",
      "iteration 3800 / 5500: loss 2.173595\n",
      "iteration 3900 / 5500: loss 2.140632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4000 / 5500: loss 2.357428\n",
      "iteration 4100 / 5500: loss 2.120059\n",
      "iteration 4200 / 5500: loss 2.355853\n",
      "iteration 4300 / 5500: loss 2.216064\n",
      "iteration 4400 / 5500: loss 2.462925\n",
      "iteration 4500 / 5500: loss 2.308882\n",
      "iteration 4600 / 5500: loss 2.203101\n",
      "iteration 4700 / 5500: loss 2.180506\n",
      "iteration 4800 / 5500: loss 2.082960\n",
      "iteration 4900 / 5500: loss 2.122108\n",
      "iteration 5000 / 5500: loss 2.084994\n",
      "iteration 5100 / 5500: loss 2.090579\n",
      "iteration 5200 / 5500: loss 2.425464\n",
      "iteration 5300 / 5500: loss 2.094774\n",
      "iteration 5400 / 5500: loss 2.130195\n",
      "lr 5.000000e-06 reg 2.500000e+04\n",
      "iteration 0 / 5500: loss 767.369864\n",
      "iteration 100 / 5500: loss 2.664116\n",
      "iteration 200 / 5500: loss 2.282791\n",
      "iteration 300 / 5500: loss 3.724040\n",
      "iteration 400 / 5500: loss 2.958273\n",
      "iteration 500 / 5500: loss 3.175456\n",
      "iteration 600 / 5500: loss 2.346292\n",
      "iteration 700 / 5500: loss 2.204232\n",
      "iteration 800 / 5500: loss 2.593049\n",
      "iteration 900 / 5500: loss 2.517462\n",
      "iteration 1000 / 5500: loss 2.921979\n",
      "iteration 1100 / 5500: loss 2.508491\n",
      "iteration 1200 / 5500: loss 2.720767\n",
      "iteration 1300 / 5500: loss 2.490981\n",
      "iteration 1400 / 5500: loss 2.877482\n",
      "iteration 1500 / 5500: loss 3.389730\n",
      "iteration 1600 / 5500: loss 2.256601\n",
      "iteration 1700 / 5500: loss 2.635946\n",
      "iteration 1800 / 5500: loss 2.213887\n",
      "iteration 1900 / 5500: loss 2.723051\n",
      "iteration 2000 / 5500: loss 2.766907\n",
      "iteration 2100 / 5500: loss 3.171244\n",
      "iteration 2200 / 5500: loss 3.109872\n",
      "iteration 2300 / 5500: loss 4.271374\n",
      "iteration 2400 / 5500: loss 2.299257\n",
      "iteration 2500 / 5500: loss 3.161425\n",
      "iteration 2600 / 5500: loss 2.647940\n",
      "iteration 2700 / 5500: loss 2.459070\n",
      "iteration 2800 / 5500: loss 2.432859\n",
      "iteration 2900 / 5500: loss 3.504312\n",
      "iteration 3000 / 5500: loss 2.287454\n",
      "iteration 3100 / 5500: loss 3.765628\n",
      "iteration 3200 / 5500: loss 2.202436\n",
      "iteration 3300 / 5500: loss 2.659985\n",
      "iteration 3400 / 5500: loss 2.641691\n",
      "iteration 3500 / 5500: loss 2.724163\n",
      "iteration 3600 / 5500: loss 2.549682\n",
      "iteration 3700 / 5500: loss 2.592799\n",
      "iteration 3800 / 5500: loss 2.826037\n",
      "iteration 3900 / 5500: loss 2.453339\n",
      "iteration 4000 / 5500: loss 2.578592\n",
      "iteration 4100 / 5500: loss 2.736723\n",
      "iteration 4200 / 5500: loss 2.280899\n",
      "iteration 4300 / 5500: loss 2.935579\n",
      "iteration 4400 / 5500: loss 2.753656\n",
      "iteration 4500 / 5500: loss 2.446140\n",
      "iteration 4600 / 5500: loss 2.175060\n",
      "iteration 4700 / 5500: loss 2.590300\n",
      "iteration 4800 / 5500: loss 3.190600\n",
      "iteration 4900 / 5500: loss 2.538130\n",
      "iteration 5000 / 5500: loss 3.348525\n",
      "iteration 5100 / 5500: loss 2.934433\n",
      "iteration 5200 / 5500: loss 2.784350\n",
      "iteration 5300 / 5500: loss 2.226010\n",
      "iteration 5400 / 5500: loss 2.391200\n",
      "lr 5.000000e-06 reg 5.000000e+04\n",
      "iteration 0 / 5500: loss 1539.517983\n",
      "iteration 100 / 5500: loss 2.785687\n",
      "iteration 200 / 5500: loss 3.583366\n",
      "iteration 300 / 5500: loss 3.923623\n",
      "iteration 400 / 5500: loss 3.542899\n",
      "iteration 500 / 5500: loss 2.588204\n",
      "iteration 600 / 5500: loss 3.695233\n",
      "iteration 700 / 5500: loss 3.611885\n",
      "iteration 800 / 5500: loss 3.443379\n",
      "iteration 900 / 5500: loss 3.932482\n",
      "iteration 1000 / 5500: loss 2.840773\n",
      "iteration 1100 / 5500: loss 3.789501\n",
      "iteration 1200 / 5500: loss 2.847215\n",
      "iteration 1300 / 5500: loss 3.674268\n",
      "iteration 1400 / 5500: loss 3.412781\n",
      "iteration 1500 / 5500: loss 4.113447\n",
      "iteration 1600 / 5500: loss 2.849347\n",
      "iteration 1700 / 5500: loss 3.645423\n",
      "iteration 1800 / 5500: loss 3.066889\n",
      "iteration 1900 / 5500: loss 2.835107\n",
      "iteration 2000 / 5500: loss 2.931285\n",
      "iteration 2100 / 5500: loss 3.183675\n",
      "iteration 2200 / 5500: loss 4.073900\n",
      "iteration 2300 / 5500: loss 3.288364\n",
      "iteration 2400 / 5500: loss 4.096474\n",
      "iteration 2500 / 5500: loss 3.617097\n",
      "iteration 2600 / 5500: loss 4.015539\n",
      "iteration 2700 / 5500: loss 3.126660\n",
      "iteration 2800 / 5500: loss 3.064169\n",
      "iteration 2900 / 5500: loss 3.997846\n",
      "iteration 3000 / 5500: loss 3.408946\n",
      "iteration 3100 / 5500: loss 3.509534\n",
      "iteration 3200 / 5500: loss 3.899007\n",
      "iteration 3300 / 5500: loss 3.348631\n",
      "iteration 3400 / 5500: loss 4.670711\n",
      "iteration 3500 / 5500: loss 3.193116\n",
      "iteration 3600 / 5500: loss 2.988788\n",
      "iteration 3700 / 5500: loss 3.272376\n",
      "iteration 3800 / 5500: loss 2.848437\n",
      "iteration 3900 / 5500: loss 3.237631\n",
      "iteration 4000 / 5500: loss 3.481448\n",
      "iteration 4100 / 5500: loss 3.564016\n",
      "iteration 4200 / 5500: loss 3.475841\n",
      "iteration 4300 / 5500: loss 3.441464\n",
      "iteration 4400 / 5500: loss 3.520612\n",
      "iteration 4500 / 5500: loss 2.566895\n",
      "iteration 4600 / 5500: loss 4.023744\n",
      "iteration 4700 / 5500: loss 4.087122\n",
      "iteration 4800 / 5500: loss 3.876792\n",
      "iteration 4900 / 5500: loss 3.007890\n",
      "iteration 5000 / 5500: loss 3.983923\n",
      "iteration 5100 / 5500: loss 3.585669\n",
      "iteration 5200 / 5500: loss 3.036782\n",
      "iteration 5300 / 5500: loss 3.296732\n",
      "iteration 5400 / 5500: loss 3.390299\n",
      "lr 5.000000e-06 reg 1.000000e+05\n",
      "iteration 0 / 5500: loss 3107.400154\n",
      "iteration 100 / 5500: loss 8.480899\n",
      "iteration 200 / 5500: loss 9.222173\n",
      "iteration 300 / 5500: loss 9.539216\n",
      "iteration 400 / 5500: loss 8.621158\n",
      "iteration 500 / 5500: loss 8.181186\n",
      "iteration 600 / 5500: loss 7.623840\n",
      "iteration 700 / 5500: loss 8.708056\n",
      "iteration 800 / 5500: loss 7.816581\n",
      "iteration 900 / 5500: loss 7.448221\n",
      "iteration 1000 / 5500: loss 8.889261\n",
      "iteration 1100 / 5500: loss 7.455828\n",
      "iteration 1200 / 5500: loss 8.853325\n",
      "iteration 1300 / 5500: loss 8.338103\n",
      "iteration 1400 / 5500: loss 10.165940\n",
      "iteration 1500 / 5500: loss 7.465852\n",
      "iteration 1600 / 5500: loss 8.011987\n",
      "iteration 1700 / 5500: loss 8.697749\n",
      "iteration 1800 / 5500: loss 9.213114\n",
      "iteration 1900 / 5500: loss 6.827696\n",
      "iteration 2000 / 5500: loss 8.834192\n",
      "iteration 2100 / 5500: loss 9.119956\n",
      "iteration 2200 / 5500: loss 8.204487\n",
      "iteration 2300 / 5500: loss 7.573577\n",
      "iteration 2400 / 5500: loss 7.770715\n",
      "iteration 2500 / 5500: loss 8.080254\n",
      "iteration 2600 / 5500: loss 9.286054\n",
      "iteration 2700 / 5500: loss 7.806999\n",
      "iteration 2800 / 5500: loss 8.855593\n",
      "iteration 2900 / 5500: loss 8.998384\n",
      "iteration 3000 / 5500: loss 8.853031\n",
      "iteration 3100 / 5500: loss 7.511324\n",
      "iteration 3200 / 5500: loss 7.576624\n",
      "iteration 3300 / 5500: loss 8.314481\n",
      "iteration 3400 / 5500: loss 8.651626\n",
      "iteration 3500 / 5500: loss 7.113780\n",
      "iteration 3600 / 5500: loss 6.843673\n",
      "iteration 3700 / 5500: loss 8.420048\n",
      "iteration 3800 / 5500: loss 7.881743\n",
      "iteration 3900 / 5500: loss 8.163933\n",
      "iteration 4000 / 5500: loss 8.185162\n",
      "iteration 4100 / 5500: loss 7.300845\n",
      "iteration 4200 / 5500: loss 9.169955\n",
      "iteration 4300 / 5500: loss 9.318774\n",
      "iteration 4400 / 5500: loss 8.146188\n",
      "iteration 4500 / 5500: loss 9.270663\n",
      "iteration 4600 / 5500: loss 8.033755\n",
      "iteration 4700 / 5500: loss 7.886130\n",
      "iteration 4800 / 5500: loss 6.852135\n",
      "iteration 4900 / 5500: loss 9.268092\n",
      "iteration 5000 / 5500: loss 8.168022\n",
      "iteration 5100 / 5500: loss 8.775709\n",
      "iteration 5200 / 5500: loss 8.228937\n",
      "iteration 5300 / 5500: loss 7.548209\n",
      "iteration 5400 / 5500: loss 8.069963\n",
      "lr 5.000000e-06 reg 2.500000e+05\n",
      "iteration 0 / 5500: loss 7675.950238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dd/code/cs231n/assignment1/cs231n/classifiers/softmax.py:99: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.log(loss)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 5500: loss inf\n",
      "iteration 200 / 5500: loss inf\n",
      "iteration 300 / 5500: loss inf\n",
      "iteration 400 / 5500: loss inf\n",
      "iteration 500 / 5500: loss inf\n",
      "iteration 600 / 5500: loss inf\n",
      "iteration 700 / 5500: loss inf\n",
      "iteration 800 / 5500: loss inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dd/code/cs231n/assignment1/cs231n/classifiers/softmax.py:101: RuntimeWarning: overflow encountered in double_scalars\n",
      "  loss += reg * np.sum(W * W)\n",
      "/home/dd/code/cs231n/assignment1/cs231n/classifiers/softmax.py:101: RuntimeWarning: overflow encountered in multiply\n",
      "  loss += reg * np.sum(W * W)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 5500: loss inf\n",
      "iteration 1000 / 5500: loss inf\n",
      "iteration 1100 / 5500: loss inf\n",
      "iteration 1200 / 5500: loss inf\n",
      "iteration 1300 / 5500: loss inf\n",
      "iteration 1400 / 5500: loss inf\n",
      "iteration 1500 / 5500: loss inf\n",
      "iteration 1600 / 5500: loss inf\n",
      "iteration 1700 / 5500: loss inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dd/code/cs231n/assignment1/cs231n/classifiers/softmax.py:96: RuntimeWarning: overflow encountered in multiply\n",
      "  dW += 2 * reg * W\n",
      "/home/dd/code/cs231n/assignment1/cs231n/classifiers/softmax.py:89: RuntimeWarning: invalid value encountered in subtract\n",
      "  S -= np.max(S, axis = 1).reshape((num_train, 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1800 / 5500: loss nan\n",
      "iteration 1900 / 5500: loss nan\n",
      "iteration 2000 / 5500: loss nan\n",
      "iteration 2100 / 5500: loss nan\n",
      "iteration 2200 / 5500: loss nan\n",
      "iteration 2300 / 5500: loss nan\n",
      "iteration 2400 / 5500: loss nan\n",
      "iteration 2500 / 5500: loss nan\n",
      "iteration 2600 / 5500: loss nan\n",
      "iteration 2700 / 5500: loss nan\n",
      "iteration 2800 / 5500: loss nan\n",
      "iteration 2900 / 5500: loss nan\n",
      "iteration 3000 / 5500: loss nan\n",
      "iteration 3100 / 5500: loss nan\n",
      "iteration 3200 / 5500: loss nan\n",
      "iteration 3300 / 5500: loss nan\n",
      "iteration 3400 / 5500: loss nan\n",
      "iteration 3500 / 5500: loss nan\n",
      "iteration 3600 / 5500: loss nan\n",
      "iteration 3700 / 5500: loss nan\n",
      "iteration 3800 / 5500: loss nan\n",
      "iteration 3900 / 5500: loss nan\n",
      "iteration 4000 / 5500: loss nan\n",
      "iteration 4100 / 5500: loss nan\n",
      "iteration 4200 / 5500: loss nan\n",
      "iteration 4300 / 5500: loss nan\n",
      "iteration 4400 / 5500: loss nan\n",
      "iteration 4500 / 5500: loss nan\n",
      "iteration 4600 / 5500: loss nan\n",
      "iteration 4700 / 5500: loss nan\n",
      "iteration 4800 / 5500: loss nan\n",
      "iteration 4900 / 5500: loss nan\n",
      "iteration 5000 / 5500: loss nan\n",
      "iteration 5100 / 5500: loss nan\n",
      "iteration 5200 / 5500: loss nan\n",
      "iteration 5300 / 5500: loss nan\n",
      "iteration 5400 / 5500: loss nan\n",
      "lr 5.000000e-08 reg 5.000000e+02 train accuracy: 0.288694 val accuracy: 0.280000\n",
      "lr 5.000000e-08 reg 1.000000e+03 train accuracy: 0.310796 val accuracy: 0.312000\n",
      "lr 5.000000e-08 reg 5.000000e+03 train accuracy: 0.372265 val accuracy: 0.396000\n",
      "lr 5.000000e-08 reg 1.000000e+04 train accuracy: 0.356041 val accuracy: 0.376000\n",
      "lr 5.000000e-08 reg 2.500000e+04 train accuracy: 0.325490 val accuracy: 0.339000\n",
      "lr 5.000000e-08 reg 5.000000e+04 train accuracy: 0.300041 val accuracy: 0.317000\n",
      "lr 5.000000e-08 reg 1.000000e+05 train accuracy: 0.285061 val accuracy: 0.296000\n",
      "lr 5.000000e-08 reg 2.500000e+05 train accuracy: 0.280633 val accuracy: 0.282000\n",
      "lr 1.000000e-07 reg 5.000000e+02 train accuracy: 0.343816 val accuracy: 0.352000\n",
      "lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.369714 val accuracy: 0.383000\n",
      "lr 1.000000e-07 reg 5.000000e+03 train accuracy: 0.374735 val accuracy: 0.390000\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.355918 val accuracy: 0.365000\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.330327 val accuracy: 0.341000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.314429 val accuracy: 0.327000\n",
      "lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.289776 val accuracy: 0.305000\n",
      "lr 1.000000e-07 reg 2.500000e+05 train accuracy: 0.276429 val accuracy: 0.287000\n",
      "lr 5.000000e-07 reg 5.000000e+02 train accuracy: 0.415469 val accuracy: 0.405000\n",
      "lr 5.000000e-07 reg 1.000000e+03 train accuracy: 0.404735 val accuracy: 0.397000\n",
      "lr 5.000000e-07 reg 5.000000e+03 train accuracy: 0.372082 val accuracy: 0.374000\n",
      "lr 5.000000e-07 reg 1.000000e+04 train accuracy: 0.346837 val accuracy: 0.370000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.322673 val accuracy: 0.341000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.298306 val accuracy: 0.316000\n",
      "lr 5.000000e-07 reg 1.000000e+05 train accuracy: 0.279551 val accuracy: 0.284000\n",
      "lr 5.000000e-07 reg 2.500000e+05 train accuracy: 0.255980 val accuracy: 0.274000\n",
      "lr 1.000000e-06 reg 5.000000e+02 train accuracy: 0.415531 val accuracy: 0.407000\n",
      "lr 1.000000e-06 reg 1.000000e+03 train accuracy: 0.402347 val accuracy: 0.400000\n",
      "lr 1.000000e-06 reg 5.000000e+03 train accuracy: 0.370143 val accuracy: 0.370000\n",
      "lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.351531 val accuracy: 0.368000\n",
      "lr 1.000000e-06 reg 2.500000e+04 train accuracy: 0.314816 val accuracy: 0.338000\n",
      "lr 1.000000e-06 reg 5.000000e+04 train accuracy: 0.283000 val accuracy: 0.301000\n",
      "lr 1.000000e-06 reg 1.000000e+05 train accuracy: 0.256490 val accuracy: 0.280000\n",
      "lr 1.000000e-06 reg 2.500000e+05 train accuracy: 0.269061 val accuracy: 0.262000\n",
      "lr 5.000000e-06 reg 5.000000e+02 train accuracy: 0.386878 val accuracy: 0.379000\n",
      "lr 5.000000e-06 reg 1.000000e+03 train accuracy: 0.354245 val accuracy: 0.360000\n",
      "lr 5.000000e-06 reg 5.000000e+03 train accuracy: 0.323388 val accuracy: 0.332000\n",
      "lr 5.000000e-06 reg 1.000000e+04 train accuracy: 0.284673 val accuracy: 0.288000\n",
      "lr 5.000000e-06 reg 2.500000e+04 train accuracy: 0.235163 val accuracy: 0.227000\n",
      "lr 5.000000e-06 reg 5.000000e+04 train accuracy: 0.185898 val accuracy: 0.191000\n",
      "lr 5.000000e-06 reg 1.000000e+05 train accuracy: 0.120878 val accuracy: 0.104000\n",
      "lr 5.000000e-06 reg 2.500000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.407000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [5e-8, 1e-7, 5e-7, 1e-6, 5e-6]\n",
    "regularization_strengths = [5e2, 1e3, 5e3, 1e4, 2.5e4, 5e4, 1e5, 2.5e5]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for learning_rate in learning_rates:\n",
    "    for regularization_strength in regularization_strengths:\n",
    "        softmax = Softmax()\n",
    "        print ('lr %e reg %e' % (learning_rate, regularization_strength))\n",
    "        softmax.train(X_train, y_train, learning_rate, regularization_strength, num_iters=5500,\n",
    "                       batch_size=200, verbose=True)\n",
    "        \n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        train_acc = np.mean(y_train == y_train_pred)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        results[(learning_rate, regularization_strength)] = (train_acc, val_acc)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_softmax = softmax\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.397000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF8CAYAAADrUz6WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9jJlMBgMBoPBcAbsZcpgMBgMBoPhDPx/E18shOobyFgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7555ba6668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
